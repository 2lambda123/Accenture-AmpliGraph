{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve, _SigmoidCalibration\n",
    "from ampligraph.evaluation import evaluate_performance, mr_score, mrr_score, hits_at_n_score, generate_corruptions_for_eval\n",
    "from sklearn.metrics import brier_score_loss, log_loss, accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "from ampligraph.datasets import load_yago39k\n",
    "from ampligraph.latent_features.models import TransE, ComplEx, DistMult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_yago39k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_pos = X['valid'][X['valid_labels']]\n",
    "X_valid_neg = X['valid'][~X['valid_labels']]\n",
    "\n",
    "X_test_pos = X['test'][X['test_labels']]\n",
    "X_test_neg = X['test'][~X['test_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm import tqdm\n",
    "from ampligraph.datasets import AmpligraphDatasetAdapter, NumpyDatasetAdapter\n",
    "from ampligraph.evaluation import generate_corruptions_for_fit, to_idx, generate_corruptions_for_eval, \\\n",
    "    hits_at_n_score, mrr_score\n",
    "\n",
    "\n",
    "def generate_corruptions(self, X_pos, batches_count, epochs):\n",
    "    try:\n",
    "        tf.reset_default_graph()\n",
    "        self.rnd = check_random_state(self.seed)\n",
    "        tf.random.set_random_seed(self.seed)\n",
    "\n",
    "        self._load_model_from_trained_params()\n",
    "\n",
    "        dataset_handle = NumpyDatasetAdapter()\n",
    "        dataset_handle.use_mappings(self.rel_to_idx, self.ent_to_idx)\n",
    "\n",
    "        dataset_handle.set_data(X_pos, \"pos\")\n",
    "\n",
    "        batch_size_pos = int(np.ceil(dataset_handle.get_size(\"pos\") / batches_count))\n",
    "\n",
    "        gen_fn = partial(dataset_handle.get_next_train_batch, batch_size=batch_size_pos, dataset_type=\"pos\")\n",
    "        dataset = tf.data.Dataset.from_generator(gen_fn,\n",
    "                                                 output_types=tf.int32,\n",
    "                                                 output_shapes=(None, 3))\n",
    "        dataset = dataset.repeat().prefetch(1)\n",
    "        dataset_iter = tf.data.make_one_shot_iterator(dataset)\n",
    "\n",
    "        x_pos_tf = dataset_iter.get_next()\n",
    "\n",
    "        e_s, e_p, e_o = self._lookup_embeddings(x_pos_tf)\n",
    "        scores_pos = self._fn(e_s, e_p, e_o)\n",
    "\n",
    "        x_neg_tf = generate_corruptions_for_fit(x_pos_tf,\n",
    "                                                entities_list=None,\n",
    "                                                eta=1,\n",
    "                                                corrupt_side='s+o',\n",
    "                                                entities_size=0,\n",
    "                                                rnd=self.seed)\n",
    "\n",
    "        e_s_neg, e_p_neg, e_o_neg = self._lookup_embeddings(x_neg_tf)\n",
    "        scores = self._fn(e_s_neg, e_p_neg, e_o_neg)\n",
    "\n",
    "        epoch_iterator_with_progress = tqdm(range(1, epochs + 1), disable=(not self.verbose), unit='epoch')\n",
    "\n",
    "        scores_list = []\n",
    "        with tf.Session(config=self.tf_config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for _ in epoch_iterator_with_progress:\n",
    "                losses = []\n",
    "                for batch in range(batches_count):\n",
    "                    scores_list.append(sess.run(scores))\n",
    "\n",
    "        dataset_handle.cleanup()\n",
    "        return np.concatenate(scores_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        dataset_handle.cleanup()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_iso(cal_model, pos_scores, neg_scores, positive_base_rate):\n",
    "    weigths_pos = len(neg_scores) / len(pos_scores)\n",
    "    weights_neg = (1.0 - positive_base_rate) / positive_base_rate\n",
    "    weights = np.concatenate((np.full(pos_scores.shape, weigths_pos),\n",
    "                              np.full(neg_scores.shape, weights_neg))).astype(float)\n",
    "    target =  np.concatenate((np.ones(pos_scores.shape), np.zeros(neg_scores.shape))).astype(float)\n",
    "    x = np.concatenate((pos_scores, neg_scores)).astype(float)\n",
    "    \n",
    "    cal_model.fit(x, target, sample_weight=weights)\n",
    "    return cal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses =  ['self_adversarial', 'pairwise', 'nll', 'multiclass_nll']\n",
    "models = [TransE, DistMult, ComplEx]\n",
    "\n",
    "results = []\n",
    "\n",
    "for m, l in itertools.product(models, losses):\n",
    "    model = m(batches_count=64, seed=0, epochs=1000, k=100, eta=20,\n",
    "                   optimizer='adam', optimizer_params={'lr':0.0001},\n",
    "                   loss=l, verbose=False)\n",
    "\n",
    "    model.fit(X['train'])\n",
    "    \n",
    "    scores = model.predict(X['test'])\n",
    "\n",
    "    model.calibrate(X_valid_pos, batches_count=10, epochs=1000, positive_base_rate=0.5)\n",
    "    print(\"pos\", model.calibration_parameters)\n",
    "    probas1 = model.predict_proba(X['test'])\n",
    "\n",
    "    model.calibrate(X_valid_pos, X_valid_neg)\n",
    "    print(\"pos neg\", model.calibration_parameters)\n",
    "    probas2 = model.predict_proba(X['test'])\n",
    "    \n",
    "    val_scores = model.predict(X['valid'])\n",
    "    ir = IsotonicRegression(out_of_bounds='clip')\n",
    "    ir.fit(np.squeeze(val_scores).astype(float), (X['valid_labels']).astype(float))\n",
    "    probas3 = ir.predict(np.squeeze(scores).astype(float))\n",
    "    \n",
    "    model.generate_corruptions = types.MethodType(generate_corruptions, model)\n",
    "    corruptions = model.generate_corruptions(X_valid_pos, batches_count=10, epochs=1000)\n",
    "    val_pos_scores = np.squeeze(model.predict(X_valid_pos))\n",
    "    iso_pos = pos_iso(IsotonicRegression(out_of_bounds='clip'), val_pos_scores, corruptions, positive_base_rate=0.5)\n",
    "    probas4 = iso_pos.predict(np.squeeze(scores).astype(float))\n",
    "\n",
    "    sc_pos = pos_iso(_SigmoidCalibration(), val_pos_scores, corruptions, positive_base_rate=0.5)\n",
    "    print(\"pos sc\", sc_pos.a_, sc_pos.b_)\n",
    "    probas5 = sc_pos.predict(np.squeeze(scores).astype(float))\n",
    "    \n",
    "    val_neg_scores = np.squeeze(model.predict(X_valid_neg))\n",
    "    sc_pos_neg = pos_iso(_SigmoidCalibration(), val_pos_scores, val_neg_scores, positive_base_rate=0.5)\n",
    "    print(\"pos neg sc\", sc_pos_neg.a_, sc_pos_neg.b_)\n",
    "    probas6 = sc_pos_neg.predict(np.squeeze(scores).astype(float))\n",
    "    \n",
    "    thresholds = {r: np.median(np.sort(val_scores[X['valid'][:, 1] == r])) for r in np.unique(X['valid'][:, 1])}\n",
    "    thresholds_test = np.vectorize(thresholds.get)(X['test'][:, 1])\n",
    "    per_relation_acc = accuracy_score(X['test_labels'], scores > thresholds_test)\n",
    "\n",
    "    acc_uncalib = accuracy_score(X['test_labels'], expit(scores) > 0.5)\n",
    "\n",
    "    acc1 = accuracy_score(X['test_labels'], probas1 > 0.5)\n",
    "    acc2 = accuracy_score(X['test_labels'], probas2 > 0.5)\n",
    "    acc3 = accuracy_score(X['test_labels'], probas3 > 0.5)\n",
    "    acc4 = accuracy_score(X['test_labels'], probas4 > 0.5)\n",
    "    acc5 = accuracy_score(X['test_labels'], probas5 > 0.5)\n",
    "    acc6 = accuracy_score(X['test_labels'], probas6 > 0.5)\n",
    "    \n",
    "    filter_triples = np.concatenate((X['train'], X_valid_pos, X_test_pos))\n",
    "    ranks = evaluate_performance(X_test_pos, \n",
    "                                 model=model, \n",
    "                                 filter_triples=filter_triples,\n",
    "                                 use_default_protocol=True, \n",
    "                                 verbose=False)\n",
    "\n",
    "    results.append({\n",
    "        'model': m.__name__,\n",
    "        'loss': l,\n",
    "        'brier_score_scores': brier_score_loss(X['test_labels'], expit(scores)),\n",
    "        'log_loss_scores': log_loss(X['test_labels'], expit(scores), eps=1e-7),\n",
    "        'brier_score_probas_pos': brier_score_loss(X['test_labels'], probas1),\n",
    "        'log_loss_probas_pos': log_loss(X['test_labels'], probas1, eps=1e-7),\n",
    "        'brier_score_probas_pos_neg': brier_score_loss(X['test_labels'], probas2),\n",
    "        'log_loss_probas_pos_neg': log_loss(X['test_labels'], probas2, eps=1e-7),\n",
    "        'brier_score_probas_pos_neg_iso': brier_score_loss(X['test_labels'], probas3),\n",
    "        'log_loss_probas_pos_neg_iso': log_loss(X['test_labels'], probas3, eps=1e-7),\n",
    "        'brier_score_probas_pos_iso': brier_score_loss(X['test_labels'], probas4),\n",
    "        'log_loss_probas_pos_iso': log_loss(X['test_labels'], probas4, eps=1e-7),\n",
    "        'brier_score_probas_pos_sc': brier_score_loss(X['test_labels'], probas5),\n",
    "        'log_loss_probas_pos_sc': log_loss(X['test_labels'], probas5, eps=1e-7),\n",
    "        'brier_score_probas_pos_neg_sc': brier_score_loss(X['test_labels'], probas6),\n",
    "        'log_loss_probas_pos_neg_sc': log_loss(X['test_labels'], probas6, eps=1e-7),\n",
    "        'metrics_mrr': mrr_score(ranks), \n",
    "        'metrics_hits@10': hits_at_n_score(ranks, n=10),\n",
    "        'metrics_mr': mr_score(ranks),\n",
    "        'accuracy_per_relation': per_relation_acc,\n",
    "        'accuracy_uncalib': acc_uncalib,\n",
    "        'accuracy_pos': acc1,\n",
    "        'accuracy_pos_neg': acc2,\n",
    "        'accuracy_pos_neg_iso': acc3,\n",
    "        'accuracy_pos_iso': acc4,\n",
    "        'accuracy_pos_sc': acc5,\n",
    "        'accuracy_pos_neg_sc': acc6\n",
    "    })\n",
    "        \n",
    "    print(json.dumps(results[-1], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['font-weight: bold' if v else '' for v in is_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).set_index(['model', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = df[(c for c in df.columns if c.startswith('brier'))]\n",
    "bs.columns = [c[len(\"brier_score_\"):] for c in bs.columns]\n",
    "bs.style.apply(highlight_min, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = df[(c for c in df.columns if c.startswith('log_loss'))]\n",
    "ll.columns = [c[len(\"log_loss_\"):] for c in ll.columns]\n",
    "ll.style.apply(highlight_min, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((bs.reset_index()\n",
    " .query(\"loss == 'self_adversarial' \")\n",
    " [['model', 'scores', 'probas_pos_neg', 'probas_pos_neg_iso', 'probas_pos', 'probas_pos_iso']]\n",
    " .reset_index(drop=True)\n",
    " .round(3)\n",
    " .to_latex()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ll.reset_index()\n",
    " .query(\"loss == 'self_adversarial' \")\n",
    " [['model', 'scores', 'probas_pos_neg', 'probas_pos_neg_iso', 'probas_pos', 'probas_pos_iso']]\n",
    " .reset_index(drop=True)\n",
    "  .round(3)\n",
    " .to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((acc*100).reset_index()\n",
    " .query(\"loss == 'self_adversarial' \")\n",
    " [['model', 'pos_neg', 'pos_neg_iso', 'pos', 'pos_iso',  'uncalib', 'per_relation']]\n",
    " .reset_index(drop=True)\n",
    "  .round(1)\n",
    " .to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = df[(c for c in df.columns if c.startswith('metrics'))]\n",
    "metrics.columns = [c[len(\"metrics_\"):] for c in metrics.columns]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_min = s == s.max()\n",
    "    return ['font-weight: bold' if v else '' for v in is_min]\n",
    "\n",
    "acc = df[(c for c in df.columns if c.startswith('accuracy'))]\n",
    "acc.columns = [c[len(\"accuracy_\"):] for c in acc.columns]\n",
    "acc.style.apply(highlight_max, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(X['valid'][:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
