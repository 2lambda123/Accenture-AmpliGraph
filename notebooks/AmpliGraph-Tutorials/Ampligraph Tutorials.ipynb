{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ampligraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fb15k_237, load_yago3_10\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_mappings, to_idx\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrr_score, hits_at_n_score, mr_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ampligraph'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:05:40.599104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 11:05:42.232213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38236 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:40:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 66ms/step - loss: 6982.9214\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6986.3037\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6987.4985\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6987.1611\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6987.0215\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6986.9375\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6987.0557\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6987.1411\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6986.9224\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 6986.8384\n",
      "Time taken: 20.63612174987793\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50,\n",
    "                                     scoring_type='Random')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 413ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005052450328641726"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 84s 406ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005052450328641726"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 52ms/step - loss: 6736.2759\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6736.0967\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6735.1055\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6730.8101\n",
      "Epoch 5/10\n",
      "     20/Unknown - 0s 23ms/step ===>..] - ETA: 0s - loss: 6755.9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:09:28.419382: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/assert_equal_1/Assert/AssertGuard/branch_executed/_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 1s 7ms/step\n",
      "29/29 [==============================] - 2s 86ms/step - loss: 6717.9346 - val_mrr: 0.0612 - val_mr: 1760.3380 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1685\n",
      "\n",
      "Epoch 5: val_mrr improved from inf to 0.06117, saving model to ./chkpt1_transe\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6688.9229\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6635.8481\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6552.2661\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 6435.7495\n",
      "Epoch 10/10\n",
      "     20/Unknown - 0s 19ms/step ===>..] - ETA: 0s - loss: 6306.7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:09:36.350125: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/assert_equal_1/Assert/AssertGuard/branch_executed/_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 1s 6ms/step\n",
      "29/29 [==============================] - 2s 84ms/step - loss: 6287.8608 - val_mrr: 0.0827 - val_mr: 886.5380 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2238\n",
      "\n",
      "Epoch 10: val_mrr did not improve from 0.06117\n",
      "Time taken: 24.376991987228394\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'mean'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=20,\n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=tf.keras.regularizers.L2(0.0001))\n",
    "\n",
    "# Use this for checkpoints at regular intervals\n",
    "checkpoint = ModelCheckpoint('./chkpt1_transe', monitor='val_mrr', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Use this for early stopping\n",
    "# checkpoint = tf.keras.callbacks.EarlyStopping(monitor=\"val_mrr\", patience=3, verbose=1, mode=\"max\", restore_best_weights=True)\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=5,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "# the training can be visualised using the following command:\n",
    "# tensorboard --logdir='./distmult_logs' --port=8891 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 84s 407ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2006679760610965, 0.14306683628535083)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and restore a model - along with the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "# explictly save the model\n",
    "save_model(model, 'saved_model_distmult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:20:27.795203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 11:20:28.392963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38236 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:40:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "# explictly save the model\n",
    "save_model(model, 'saved_model_distmult')\n",
    "\n",
    "from ampligraph.utils import restore_model\n",
    "\n",
    "# restore saved models or checkpoints\n",
    "model = restore_model('saved_model_distmult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 84s 407ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2006679760610965, 0.14306683628535083)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 63ms/step - loss: 4502.2134\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 4351.5942\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 4200.2559\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 4050.8831\n",
      "Epoch 5/10\n",
      "     20/Unknown - 0s 24ms/step ===>..] - ETA: 0s - loss: 3927.9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:22:18.395235: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/assert_equal_1/Assert/AssertGuard/branch_executed/_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 1s 7ms/step\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 3904.6897 - val_mrr: 0.0851 - val_mr: 821.9353 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2315\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 3762.6685\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 3626.8887\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 3496.4949\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 3373.1560\n",
      "Epoch 10/10\n",
      "     22/Unknown - 0s 19ms/step ===>..] - ETA: 0s - loss: 3266.5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:22:25.061613: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/assert_equal_1/Assert/AssertGuard/branch_executed/_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 1s 6ms/step\n",
      "29/29 [==============================] - 2s 82ms/step - loss: 3256.4666 - val_mrr: 0.0873 - val_mr: 762.8443 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f782816f640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can continue training from where you left after restoring the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./distmult_logs')\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=5,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[ tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 83s 401ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2082482400386288, 0.14817986104315492)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './small_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings can be visualised using the following command:\n",
    "# tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 379ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0002578554147561859, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters\n",
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       batch_size=3,\n",
    "                       use_filter={'train': dataset['train'],\n",
    "                                  'test': dataset['test']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "42/42 [==============================] - 78s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9321508953909384, 0.8788286525100303)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters (filters are file names instead of numpy arrays)\n",
    "# corruptions generated using entities subset\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=500,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1) # will give very high mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "     25/Unknown - 0s 15ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:25:34.889720: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/assert_equal_1/Assert/AssertGuard/branch_executed/_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08672618859994677, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o')\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 83s 401ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2082482400386288, 0.14817986104315492)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 83s 401ms/step\n",
      "Time taken: 92.09537172317505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2082482400386288, 0.14817986104315492)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above but just for sanity checking if entities_subset works or not\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=model.data_indexer.backend.get_all_entities(),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.4349215 -1.7000058 -1.2922773 ...  5.0519047  5.09084    5.1300077]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "# calibrate on the test set\n",
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.40698946 0.4544594  0.48121542 ... 0.83165896 0.8330912  0.8345221 ]\n",
      "[ 3834 15798 13190 ...  2021  9828  6237]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.4349215 -2.4349215 -1.7000058 ...  5.09084    5.1300077  5.1300077]\n",
      "[24272  3834 15798 ... 30266  6237 26675]\n"
     ]
    }
   ],
   "source": [
    "# check if the sorted probability indices match the sorted regular scores \n",
    "# It should be same as calibration doesnt change ranking, it just calibrates the range of scores\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 83s 402ms/step\n",
      "Time taken: 92.01281237602234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(633.1031901360211,\n",
       " 0.2082482400386288,\n",
       " 0.14817986104315492,\n",
       " 0.3218514531754575,\n",
       " 20438)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration should not affect the regular evaluation\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save just the model weights, but no other states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load just the saved weights and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:50:38.617591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 11:50:39.255278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38236 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:40:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 412ms/step\n",
      "Time taken: 96.70207262039185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(633.1031901360211,\n",
       " 0.2082482400386288,\n",
       " 0.14817986104315492,\n",
       " 0.3218514531754575,\n",
       " 20438)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loaded the model should return exact same results as earlier\n",
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=20,\n",
    "                                     scoring_type='DistMult')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[0.93889403 0.6170491  3.8736389  ... 0.6355903  0.01623714 3.0563748 ]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3834, 15798, 13190, ...,  2021,  9828,  6237]),\n",
       " array([ 3834, 15798, 13190, ...,  2021,  9828,  6237]),\n",
       " array([0.40698946, 0.4544594 , 0.48121542, ..., 0.83165896, 0.8330912 ,\n",
       "        0.8345221 ], dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted calibration scores order and regular predict scores order must match\n",
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                 batch_size=10000)\n",
    "np.argsort(out), \\\n",
    "np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), \\\n",
    "np.sort(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 0.0Bytes, after: 12.81MB, consumed: 12.81MB; exec time: 20.264s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:48:59.955534: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 14:49:00.580059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38238 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:39:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 26s 96ms/step - loss: 1095.1127\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 26s 93ms/step - loss: 1090.9618\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 26s 96ms/step - loss: 1080.2867\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 1052.5120\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 26s 93ms/step - loss: 1004.9535\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 25s 93ms/step - loss: 945.0866\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 880.6619\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 27s 97ms/step - loss: 817.8425\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 759.7907\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 26s 94ms/step - loss: 707.4430\n",
      "281.769056558609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=10)\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 227s 4s/step\n",
      "Time taken: 227.30635476112366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1171.3525540659555, 0.07391564339336283, 0.0, 0.20427634797925434, 20438)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 226s 4s/step\n",
      "Time taken: 225.89381051063538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1171.3525540659555, 0.07391564339336283, 0.0, 0.20427634797925434, 20438)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 31.779MB, after: 44.602MB, consumed: 12.823MB; exec time: 150.55s\n",
      "Epoch 1/10\n",
      "     10/Unknown - 1s 112ms/step - loss: 9905.5361 2 2\n",
      "     12/Unknown - 3s 261ms/step - loss: 10093.00002 2\n",
      "     13/Unknown - 4s 337ms/step - loss: 10167.41502 2\n",
      "     23/Unknown - 6s 258ms/step - loss: 9672.1377 2 2\n",
      "     28/Unknown - 7s 261ms/step - loss: 9846.09282 2\n",
      "     31/Unknown - 9s 279ms/step - loss: 9960.09382 2\n",
      "32/32 [==============================] - 9s 292ms/step - loss: 9643.4775\n",
      "Epoch 2/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9702.43952 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9733.74512 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9789.68852 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9655.31642 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9737.72852 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9799.10842 2\n",
      "32/32 [==============================] - 8s 257ms/step - loss: 9643.3975\n",
      "Epoch 3/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9676.61912 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9694.80372 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9728.36132 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9650.80182 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9704.71882 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9746.45902 2\n",
      "32/32 [==============================] - 8s 263ms/step - loss: 9643.2178\n",
      "Epoch 4/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9666.33592 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9679.14452 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9703.08692 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9648.49222 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9688.49322 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9720.04492 2\n",
      "32/32 [==============================] - 8s 265ms/step - loss: 9642.8262\n",
      "Epoch 5/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9660.50682 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9670.38572 2\n",
      "13/32 [===========>..................] - ETA: 5s - loss: 9688.98442 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9646.65822 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9678.32712 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9703.59382 2\n",
      "32/32 [==============================] - 9s 268ms/step - loss: 9641.9248\n",
      "Epoch 6/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9656.08792 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9664.11332 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9679.28912 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9644.31932 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9670.29002 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9691.18072 2\n",
      "32/32 [==============================] - 8s 262ms/step - loss: 9639.8564\n",
      "Epoch 7/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9651.30862 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9658.02732 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9670.78612 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9640.27442 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9661.87502 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9679.37502 2\n",
      "32/32 [==============================] - 8s 262ms/step - loss: 9635.4365\n",
      "Epoch 8/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9644.34862 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9650.05082 2\n",
      "13/32 [===========>..................] - ETA: 5s - loss: 9660.94432 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9632.76462 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9650.60162 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9665.19142 2\n",
      "32/32 [==============================] - 8s 265ms/step - loss: 9626.8018\n",
      "Epoch 9/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9632.88092 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9637.70702 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9647.02442 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9619.36722 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9633.63282 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9645.48342 2\n",
      "32/32 [==============================] - 8s 262ms/step - loss: 9611.4277\n",
      "Epoch 10/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9614.05862 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9618.05472 2\n",
      "13/32 [===========>..................] - ETA: 5s - loss: 9625.91802 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9597.08592 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9607.67772 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9616.76862 2\n",
      "32/32 [==============================] - 8s 264ms/step - loss: 9586.2090\n",
      "267.40865206718445\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs_split')\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[tensorboard_callback])\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:, 0]).union(set(dataset['train'][:, 2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 230s 4s/step\n",
      "Time taken: 230.9751660823822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1335.5136999706428, 0.06652368353952463, 0.0, 0.1842646051472747, 20438)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 431s 8s/step\n",
      "Time taken: 451.3540623188019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1194.4257021234955,\n",
       " 0.15864779365563283,\n",
       " 0.10859673157843233,\n",
       " 0.2576817692533516,\n",
       " 20438)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model with partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='Random')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[])\n",
    "print((time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=100,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='entity_frequency', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 6s 128ms/step - loss: 17856.8340\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17844.8906\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17735.7676\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17341.3867\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 16681.2773\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 15790.5664\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 14691.8779\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 5s 108ms/step - loss: 13537.4414\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 12469.3301\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 5s 111ms/step - loss: 11533.4336\n",
      "(array([0, 1, 2, 3, 4, 5], dtype=int32), array([ 49,   1,  56,  54,  43, 110]))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n",
    "print(np.unique(clusters, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 15612.8779\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 15610.4873\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15607.6924\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15603.9346\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 15598.6699\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15591.2236\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15580.7832\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 15566.3682\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15546.8447\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15521.0059\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15487.4824\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15444.7588\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 15391.5029\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 15326.3516\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15248.0615\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15155.7275\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15048.4082\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 14926.0830\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 14788.2090\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 14635.9170\n",
      "        movie_name  year\n",
      "841         Tattah  2013\n",
      "840         Tattah  2013\n",
      "1677  Re-Generator  2010\n",
      "1676  Re-Generator  2010\n",
      "2477     Ambulance  2005\n",
      "2476     Ambulance  2005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 25ms/step - loss: 496.0767\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 495.4716\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 494.6113\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 493.0753\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 490.1469\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 484.6407\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 474.8754\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 458.7555\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 434.9001\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 407.1082\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 378.7333\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 351.8762\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 327.4932\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 305.6736\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 286.2159\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 268.8826\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 253.4297\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 239.6431\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 227.2515\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 216.0930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'The Vale'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Goodbrother of Hammerhorn'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Locke of Oldcastle'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Greyjoy of Pyke']],\n",
       "       dtype='<U44'),\n",
       " array([2.2265291 , 0.5242001 , 0.5207645 , 0.48392776, 0.46780267],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='DistMult')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.8",
   "language": "python",
   "name": "tf2.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
