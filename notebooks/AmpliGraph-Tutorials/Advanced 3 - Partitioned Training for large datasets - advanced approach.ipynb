{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d979c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import numpy as np\n",
    "import ampligraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebeec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "from ampligraph.datasets.sqlite_adapter import SQLiteAdapter\n",
    "from ampligraph.datasets.graph_data_loader import GraphDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ed1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph loader - loads the data from the file, numpy array, etc and generates batchs for iterating\n",
    "# Internally it will first map raw data to indices and store in db.\n",
    "# then this will map the raw triples to indices and store in another db\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                  backend=SQLiteAdapter, # type of backend to use\n",
    "                                  batch_size=1000,       # batch size to use while iterating over this dataset\n",
    "                                  dataset_type='train',  # dataset type\n",
    "                                  use_filter=False,      # Whether to use filter or not\n",
    "                                  use_indexer=True)      # indicates that the data needs to be mapped to index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24cb1627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 864.0Bytes, after: 12.827MB, consumed: 12.826MB; exec time: 19.218s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner - in this case we choose RandomEdges partitioner\n",
    "from ampligraph.datasets import RandomEdgesGraphPartitioner\n",
    "partitioner = RandomEdgesGraphPartitioner(dataset_loader, k=3)\n",
    "\n",
    "# the above code will create a partitioner by passing the graph dataloader object\n",
    "# the partitioner will partition the data and will internally create multiple graph \n",
    "# data loaders for each partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "badb504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 1095.1127\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 1090.9618\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 25s 91ms/step - loss: 1080.2867\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 1052.5120\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 24s 89ms/step - loss: 1004.9535\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 945.0866\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 880.6620\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 25s 91ms/step - loss: 817.8426\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 24s 89ms/step - loss: 759.7908\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 25s 90ms/step - loss: 707.4431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f255998f040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and compile a model as usual\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "partitioned_model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "partitioned_model.fit(partitioner,            # pass the partitioner object as input to the fit function\n",
    "                                              # this will generate data for the model during training\n",
    "                                              # No need to pass partitioning_k parameter as this will be \n",
    "                                              # overridden by partitioner_k of input partitioner\n",
    "                      epochs=10)              # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d55c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "# create an instance of graph(triple) loader for test set by passing sql backend. \n",
    "# the data will be indexed using the models training indexer\n",
    "# and the indexed triples will be stored in a database in chunks\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "            backend=SQLiteAdapter,     # type of backend to use\n",
    "            batch_size=400,            # batch size to use while iterating over this dataset\n",
    "            dataset_type='test',       # dataset type\n",
    "            use_indexer=partitioned_model.data_indexer)    # get the data_indexer from the trained model \n",
    "                                                                        # and map the concepts to same indices \n",
    "                                                                        # as used during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f32d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 216s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1171.3521381739897, 0.0739156438083333, 0.0, 0.20427634797925434, 20438)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = partitioned_model.evaluate(dataset_loader_test, # pass the dataloader object as input to the \n",
    "                                                        # evaluate function. this will generate data\n",
    "                                                        # for the model during training\n",
    "                                   batch_size=400)\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65ef7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path ./partitioned_model_random_edges already exists. This save operation will overwrite the model                 at the specified path.\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "save_model(model=partitioned_model, model_name_path='./partitioned_model_random_edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5813a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import restore_model\n",
    "model = restore_model('./partitioned_model_random_edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c77a8342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "            backend=SQLiteAdapter,     # type of backend to use\n",
    "            batch_size=400,            # batch size to use while iterating over this dataset\n",
    "            dataset_type='test',       # dataset type\n",
    "            use_indexer=model.data_indexer)    # get the mapper from the trained model \n",
    "                                                                        # and map the concepts to same indices \n",
    "                                                                        # as used during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21abb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 215s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1171.3521381739897, 0.0739156438083333, 0.0, 0.20427634797925434, 20438)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = model.evaluate(dataset_loader_test, # pass the dataloader object as input to the \n",
    "                                                        # evaluate function. this will generate data\n",
    "                                                        # for the model during training\n",
    "                                   batch_size=400)\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b49a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.8",
   "language": "python",
   "name": "tf2.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
