{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.4'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 6982.9214\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6982.92139, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6986.3037\n",
      "\n",
      "Epoch 00002: loss did not improve from 6982.92139\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6987.4985\n",
      "\n",
      "Epoch 00003: loss did not improve from 6982.92139\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6987.1611\n",
      "\n",
      "Epoch 00004: loss did not improve from 6982.92139\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6987.0215\n",
      "\n",
      "Epoch 00005: loss did not improve from 6982.92139\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6986.9375\n",
      "\n",
      "Epoch 00006: loss did not improve from 6982.92139\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6987.0557\n",
      "\n",
      "Epoch 00007: loss did not improve from 6982.92139\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6987.1411\n",
      "\n",
      "Epoch 00008: loss did not improve from 6982.92139\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6986.9224\n",
      "\n",
      "Epoch 00009: loss did not improve from 6982.92139\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 6986.8384\n",
      "\n",
      "Epoch 00010: loss did not improve from 6982.92139\n",
      "Time taken: 2.653794288635254\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50,\n",
    "                                     scoring_type='Random')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=25,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 63s 307ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005047810301817834"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a TransE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 38ms/step - loss: 6643.7642\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6643.76416, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 6524.1372\n",
      "\n",
      "Epoch 00002: loss improved from 6643.76416 to 6524.13721, saving model to ./chkpt1\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 6375.6538\n",
      "\n",
      "Epoch 00003: loss improved from 6524.13721 to 6375.65381, saving model to ./chkpt1\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 6191.3755\n",
      "\n",
      "Epoch 00004: loss improved from 6375.65381 to 6191.37549, saving model to ./chkpt1\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 5990.8403\n",
      "\n",
      "Epoch 00005: loss improved from 6191.37549 to 5990.84033, saving model to ./chkpt1\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 5789.7212\n",
      "\n",
      "Epoch 00006: loss improved from 5990.84033 to 5789.72119, saving model to ./chkpt1\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 5595.8433\n",
      "\n",
      "Epoch 00007: loss improved from 5789.72119 to 5595.84326, saving model to ./chkpt1\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 5412.9565\n",
      "\n",
      "Epoch 00008: loss improved from 5595.84326 to 5412.95654, saving model to ./chkpt1\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 5240.8589\n",
      "\n",
      "Epoch 00009: loss improved from 5412.95654 to 5240.85889, saving model to ./chkpt1\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 5080.1538\n",
      "\n",
      "Epoch 00010: loss improved from 5240.85889 to 5080.15381, saving model to ./chkpt1\n",
      "Time taken: 4.028819561004639\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./transe_logs')\n",
    "\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50,\n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=25,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint, tensorboard_callback])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './small_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings can be visualised using the following command:\n",
    "# tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# evaluate using filters\n",
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       use_filter={'train': dataset['train']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5080, 11170],\n",
       "       [12839, 12575],\n",
       "       [ 7409,  9715]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 55s 266ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9297122190690545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters (filters are file names instead of numpy arrays)\n",
    "# corruptions generated using entities subset\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks) # will give very high mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 60s 291ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17401659700409594"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 62s 299ms/step\n",
      "Time taken: 61.74235558509827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17401659700409594"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above but just for sanity checking if entities_subset works or not\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=list(model.data_indexer.backend.entities_dict.values()),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-1.9735914e+00 -1.9564396e+00 -1.9247011e+00 ... -7.0771109e-04\n",
      " -7.0771069e-04 -7.0771034e-04]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "# calibrate on the test set\n",
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.5706937 0.5706937 0.5706937 ... 0.664212  0.6656726 0.6664607]\n",
      "[ 3581 18100   105 ... 18952 10367  1919]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-1.9735914e+00 -1.9564396e+00 -1.9247011e+00 ... -7.0771109e-04\n",
      " -7.0771069e-04 -7.0771034e-04]\n",
      "[ 1919 10367 18952 ... 11458  1247 17056]\n"
     ]
    }
   ],
   "source": [
    "# check if the sorted probability indices match the sorted regular scores \n",
    "# It should be same as calibration doesnt change ranking, it just calibrates the range of scores\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 61s 297ms/step\n",
      "Time taken: 61.1954927444458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(541.0079264115863,\n",
       " 0.17401659700409594,\n",
       " 0.11341618553674528,\n",
       " 0.2935952637244349,\n",
       " 20438)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration should not affect the regular evaluation\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 60s 293ms/step\n",
      "Time taken: 60.51377248764038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(541.0079264115863,\n",
       " 0.17401659700409594,\n",
       " 0.11341618553674528,\n",
       " 0.2935952637244349,\n",
       " 20438)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loaded the model should return exact same results as earlier\n",
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='TransE')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-0.11748545 -0.05667515 -0.00765535 ... -0.10156396 -1.5409082\n",
      " -0.01155617]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3581, 18100,   105, ..., 18952, 10367,  1919]),\n",
       " array([ 1919, 10367, 18952, ..., 11458,  1247, 17056]),\n",
       " array([0.5706937, 0.5706937, 0.5706937, ..., 0.664212 , 0.6656726,\n",
       "        0.6664607], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted calibration scores order and regular predict scores order must match\n",
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "np.argsort(out), np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), np.sort(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 896.0Bytes, after: 12.929MB, consumed: 12.928MB; exec time: 31.597s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 1034.5422\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 980.0396\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 17s 62ms/step - loss: 931.1979\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 17s 62ms/step - loss: 885.2751\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 842.0829\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 17s 62ms/step - loss: 801.2522\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 762.9703\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 17s 62ms/step - loss: 727.4066\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 17s 62ms/step - loss: 694.4910\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 664.2192\n",
      "183.84469532966614\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 122s 2s/step\n",
      "Time taken: 121.93693733215332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(990.8508904980918, 0.08935615089071472, 0.0, 0.2461591153733242, 20438)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=DummyBackend,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 122s 2s/step\n",
      "Time taken: 122.09048652648926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(990.8508904980918, 0.08935615089071472, 0.0, 0.2461591153733242, 20438)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.0001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 47.273MB, after: 59.971MB, consumed: 12.698MB; exec time: 85.994s\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 9982.3555\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 7s 212ms/step - loss: 9970.1201\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9962.7900\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9955.9092\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 7s 221ms/step - loss: 9949.1191\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9942.3730\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9935.6865\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 7s 218ms/step - loss: 9928.9971\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9922.2402\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9915.5059\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9908.7646\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9901.9551\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9895.0830\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 7s 218ms/step - loss: 9888.1367\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 7s 222ms/step - loss: 9881.1260\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9874.0654\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9866.8926\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9859.6514\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9852.3398\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9844.9229\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9837.3740\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9829.7705\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9822.0488\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9814.2090\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9806.2617\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9798.2344\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9790.0859\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9781.8037\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9773.3887\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9764.8486\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9756.1553\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 7s 221ms/step - loss: 9747.3555\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9738.3877\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9729.2627\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9719.9805\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9710.5820\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9700.9600\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9691.2041\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9681.3496\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9671.3496\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9661.2188\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9650.9619\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9640.5342\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9630.0645\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9619.5137\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9608.7793\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9597.9414\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9587.0107\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 7s 221ms/step - loss: 9575.9727\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 9564.8389\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9553.6504\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9542.3984\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9531.0322\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9519.6035\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9508.1035\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9496.5605\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9484.9541\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9473.2773\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9461.5234\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9449.7266\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9437.8877\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9426.0225\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9414.1035\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9402.1143\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 7s 221ms/step - loss: 9390.1084\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9378.1133\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9366.0225\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9353.9463\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 7s 218ms/step - loss: 9341.8818\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9329.7969\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9317.6650\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9305.5176\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9293.3760\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9281.2324\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9269.0723\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9256.9150\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9244.7402\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9232.5654\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9220.4072\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9208.2529\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9196.0811\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 7s 222ms/step - loss: 9183.9160\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 9171.7520\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9159.6025\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9147.4189\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 9135.2520\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9123.1357\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9110.9727\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9098.8428\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 7s 218ms/step - loss: 9086.7520\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 9074.6611\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9062.6084\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9050.5566\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 7s 222ms/step - loss: 9038.5322\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 7s 216ms/step - loss: 9026.5068\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 9014.4756\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 9002.4824\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 8990.5293\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 7s 222ms/step - loss: 8978.5674\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 8966.6484\n",
      "770.9260272979736\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs_split')\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=100, callbacks=[tensorboard_callback])\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:, 0]).union(set(dataset['train'][:, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 123s 2s/step\n",
      "Time taken: 123.25823903083801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1158.5643898620217, 0.08530392588433558, 0.0, 0.22668558567374497, 20438)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 324s 6s/step\n",
      "Time taken: 323.94514298439026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1032.1129024366376,\n",
       " 0.18253165803137616,\n",
       " 0.12374009198551718,\n",
       " 0.3002984636461493,\n",
       " 20438)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model with partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 98.814MB, after: 111.52MB, consumed: 12.703MB; exec time: 87.679s\n",
      "Epoch 1/10\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 10215.4414\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 10215.4443\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 6s 210ms/step - loss: 10217.1836\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 10215.7334\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 6s 208ms/step - loss: 10215.6689\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 10215.6406\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 6s 208ms/step - loss: 10216.1572\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 10215.9971\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 10215.5830\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 6s 207ms/step - loss: 10215.2158\n",
      "162.06040239334106\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.0001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='Random')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[])\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 125s 2s/step\n",
      "Time taken: 125.02911829948425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7279.448282610823, 0.0005559095188126157, 0.0, 0.0004892846658185732, 20438)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 70ms/step - loss: 17412.5391\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 17398.1309\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 17296.1465\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 16878.4785\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 15909.9961\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 14546.6768\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 13176.4453\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 11976.7051\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 10962.2529\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 10104.8604\n",
      "Time taken: 8.281585216522217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['/m/0fvf9q', '/location/country/form_of_government', '/m/0m313']],\n",
       "       dtype=object),\n",
       " array([36.]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 2s 44ms/step - loss: 17856.8047\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 17844.0449\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 17730.1875\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 17330.3867\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 16672.2656\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 15780.3242\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 14677.3369\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 13518.6426\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 12447.1543\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 11510.4658\n",
      "(array([0, 1, 2, 3, 4, 5], dtype=int32), array([ 56,   7,  25, 154,  22,  49]))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n",
    "print(np.unique(clusters, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 15612.8799\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15610.5010\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15607.7412\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15604.0674\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15598.9365\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15591.7188\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 15581.6055\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15567.6807\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 15548.8184\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15523.8721\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 15491.4668\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15450.2480\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15398.7607\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15335.4668\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15259.4141\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15169.6143\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15065.2783\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 14945.5713\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 14810.6758\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 14660.7256\n",
      "                                             movie_name  year\n",
      "2259         Pirates of the Caribbean: Dead Man's Chest  2006\n",
      "2101           Pirates of the Caribbean: At World's End  2007\n",
      "125   Rod the Stormtrooper: Episode V - The Hidden D...  2015\n",
      "126   Rod the Stormtrooper: Episode V - The Hidden D...  2015\n",
      "6820                       Tarzan and the Green Goddess  1938\n",
      "6819                       Tarzan and the Green Goddess  1938\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 25ms/step - loss: 491.7411\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 486.8790\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 483.2306\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 480.0301\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 477.2192\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 474.6082\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 472.1904\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 469.9152\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 467.8103\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 465.7492\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 463.8291\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 461.9291\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 460.1179\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 458.3310\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 456.6570\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 455.0156\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 453.3794\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 451.8036\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 450.2982\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 448.8355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Frey of the Crossing'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH',\n",
       "         \"House Targaryen of King's Landing\"],\n",
       "        ['Eddard Stark', 'ALLIED_WITH',\n",
       "         'House Lannister of Casterly Rock'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Greyjoy of Pyke']],\n",
       "       dtype='<U44'),\n",
       " array([-1.0268682, -1.051951 , -1.0546845, -1.1503534, -1.1699506],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adagrad', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
