{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.4'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 57ms/step - loss: 6651.7305\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6651.73047, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 6546.4531\n",
      "\n",
      "Epoch 00002: loss improved from 6651.73047 to 6546.45312, saving model to ./chkpt1\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 6421.3564\n",
      "\n",
      "Epoch 00003: loss improved from 6546.45312 to 6421.35645, saving model to ./chkpt1\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 6267.3594\n",
      "\n",
      "Epoch 00004: loss improved from 6421.35645 to 6267.35938, saving model to ./chkpt1\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 6093.7065\n",
      "\n",
      "Epoch 00005: loss improved from 6267.35938 to 6093.70654, saving model to ./chkpt1\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 5913.9697\n",
      "\n",
      "Epoch 00006: loss improved from 6093.70654 to 5913.96973, saving model to ./chkpt1\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 5736.6895\n",
      "\n",
      "Epoch 00007: loss improved from 5913.96973 to 5736.68945, saving model to ./chkpt1\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 5566.1343\n",
      "\n",
      "Epoch 00008: loss improved from 5736.68945 to 5566.13428, saving model to ./chkpt1\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 5403.6733\n",
      "\n",
      "Epoch 00009: loss improved from 5566.13428 to 5403.67334, saving model to ./chkpt1\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 5249.7432\n",
      "\n",
      "Epoch 00010: loss improved from 5403.67334 to 5249.74316, saving model to ./chkpt1\n",
      "Time taken: 4.161166667938232\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=50,\n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=25,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint, tensorboard_callback])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './small_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings can be visualised using the following command:\n",
    "# tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 936ms/step\n"
     ]
    }
   ],
   "source": [
    "# evaluate using filters\n",
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       use_filter={'train': dataset['train']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using filters (filters are file names instead of numpy arrays)\n",
    "# corruptions generated using entities subset\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks) # will give very high mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 69s 334ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28979677376532464"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 70s 342ms/step\n",
      "Time taken: 70.50386095046997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28979677376532464"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above but just for sanity checking if entities_subset works or not\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=list(model.data_indexer.backend.entities_dict.values()),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.6117423 -2.5582662 -2.434757  ... 30.218779  31.553257  41.351562 ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "# calibrate on the test set\n",
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.4154276  0.41815606 0.42447674 ... 0.9985715  0.9989201  0.9998617 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.6117423 -2.5582662 -2.434757  ... 30.218779  31.553257  41.351562 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n"
     ]
    }
   ],
   "source": [
    "# check if the sorted probability indices match the sorted regular scores \n",
    "# It should be same as calibration doesnt change ranking, it just calibrates the range of scores\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 66s 321ms/step\n",
      "Time taken: 66.17234683036804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.0980771112633,\n",
       " 0.22008817445328427,\n",
       " 0.13516488893238085,\n",
       " 0.39289558665231433,\n",
       " 20438)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration should not affect the regular evaluation\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 66s 321ms/step\n",
      "Time taken: 66.22395181655884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.0980771112633,\n",
       " 0.22008817445328427,\n",
       " 0.13516488893238085,\n",
       " 0.39289558665231433,\n",
       " 20438)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loaded the model should return exact same results as earlier\n",
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='ComplEx')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[14.611031   9.369467   7.550662  ...  5.5516796  0.9343726  5.4705048]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([0.4154276 , 0.41815606, 0.42447674, ..., 0.9985715 , 0.9989201 ,\n",
       "        0.9998617 ], dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted calibration scores order and regular predict scores order must match\n",
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "np.argsort(out), np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), np.sort(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 896.0Bytes, after: 12.916MB, consumed: 12.915MB; exec time: 31.859s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 1587.2365\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 1413.6970\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 1251.7213\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 1116.2098\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 1007.5591\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 920.2757\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 849.2607\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 790.6967\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 741.4282\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 699.5930\n",
      "196.18159699440002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 128s 2s/step\n",
      "Time taken: 128.40124201774597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(464.99716214893823, 0.09894986543567783, 0.0, 0.2728251296604364, 20438)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=DummyBackend,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 128s 2s/step\n",
      "Time taken: 128.0037112236023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(464.99716214893823, 0.09894986543567783, 0.0, 0.2728251296604364, 20438)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.0001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 316.36MB, after: 296.09MB, consumed: -2.1259e+07Bytes; exec time: 87.255s\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 8s 266ms/step - loss: 16094.2588\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 16101.5303\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 8s 266ms/step - loss: 16103.0107\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 16099.6152\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 8s 260ms/step - loss: 16092.4268\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 16082.4639\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 8s 266ms/step - loss: 16070.2266\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 8s 257ms/step - loss: 16056.2168\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 16040.7246\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 16023.9854\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 8s 260ms/step - loss: 16006.1777\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 15987.3301\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 15967.5928\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 8s 261ms/step - loss: 15947.1094\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 15925.8730\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 8s 256ms/step - loss: 15903.8408\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 15881.1309\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 8s 267ms/step - loss: 15857.7402\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 8s 264ms/step - loss: 15833.7705\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 15808.9990\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 15783.5566\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 15757.5371\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 8s 265ms/step - loss: 15730.8506\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 15703.6299\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 15675.7617\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 15647.3906\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 15618.5098\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 8s 261ms/step - loss: 15589.1943\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 15559.4424\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 15529.2646\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 8s 258ms/step - loss: 15498.6572\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 15467.6758\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 15436.2578\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 15404.5742\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 15372.5420\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 15340.2090\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 15307.5547\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 15274.7012\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 8s 263ms/step - loss: 15241.5889\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 15208.2178\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 8s 259ms/step - loss: 15174.6553\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 8s 262ms/step - loss: 15140.9248\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 8s 260ms/step - loss: 15106.9512\n",
      "Epoch 44/100\n",
      "11/31 [=========>....................] - ETA: 1s - loss: 15106.3896"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-03fed0506529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n\u001b[1;32m      4\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_partitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                      epochs=100, callbacks=[tensorboard_callback])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, initial_epoch, validation_batch_size, validation_freq, validation_filter, use_partitioning)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                         \u001b[0;31m# process this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                         \u001b[0;31m# after a batch is processed call this callback function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    430\u001b[0m               \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             '''\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2574\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2575\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2577\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs_split')\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=100, callbacks=[tensorboard_callback])\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:, 0]).union(set(dataset['train'][:, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "      6/Unknown - 14s 2s/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-fd58a5b75384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n\u001b[0;32m----> 3\u001b[0;31m                                    batch_size=400)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, batch_size, verbose, use_filter, corrupt_side, entities_subset, callbacks)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0;31m# process this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                     \u001b[0moverall_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m                     \u001b[0;31m# increment the rank by 1 (ranks returned are from (0 - n-1) so increment by 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                     \u001b[0moverall_rank\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mtest_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_partitioned_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_model_inputs_for_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# run the loop based on number of parts in which the original emb matrix was generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mprocess_model_inputs_for_test\u001b[0;34m(self, triples)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0mobj_emb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0mrel_emb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ent_partition'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ment_emb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rel_partition'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrel_emb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp_triples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/dumb.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m    327\u001b[0m         warnings.warn(\"Flag must be one of 'r', 'w', 'c', or 'n'\",\n\u001b[1;32m    328\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_Database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/dumb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filebasename, mode, flag)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Handle the creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/dumb.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_and_siz_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_and_siz_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "      6/Unknown - 35s 6s/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c3dda4ae6715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n\u001b[1;32m      6\u001b[0m                               \u001b[0;34m'valid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                               'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, batch_size, verbose, use_filter, corrupt_side, entities_subset, callbacks)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0;31m# process this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                     \u001b[0moverall_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m                     \u001b[0;31m# increment the rank by 1 (ranks returned are from (0 - n-1) so increment by 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                     \u001b[0moverall_rank\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mtest_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# get the embedding matrix along with entity ids of first and last row of emb matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 emb_mat, start_ent_id, end_ent_id = self.get_emb_matrix_test(j, \n\u001b[0;32m--> 874\u001b[0;31m                                                                              number_of_parts)\n\u001b[0m\u001b[1;32m    875\u001b[0m                 \u001b[0;31m# compute the rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                 ranks = self._get_ranks(inputs, emb_mat, \n",
      "\u001b[0;32m~/code/ampligraph_projects/tf2/AmpliGraph-Lab/ampligraph/latent_features/models/ScoringBasedEmbeddingModel.py\u001b[0m in \u001b[0;36mget_emb_matrix_test\u001b[0;34m(self, part_number, number_of_parts)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ent_partition'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ment_partition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_partition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber_of_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_number\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpart_number\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/dumb.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m    327\u001b[0m         warnings.warn(\"Flag must be one of 'r', 'w', 'c', or 'n'\",\n\u001b[1;32m    328\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_Database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/dumb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filebasename, mode, flag)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Handle the creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/dbm/dumb.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_and_siz_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_and_siz_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.5/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n",
    "print(np.unique(clusters, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adagrad', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
