{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.4'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 62ms/step - loss: 6982.9214\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6982.92139, saving model to ./chkpt1\n",
      "Time taken: 13.374548196792603\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50,\n",
    "                                     scoring_type='Random')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=1,\n",
    "             validation_freq=25,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 108s 522ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005052450328641726"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 64ms/step - loss: 6736.0894\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6736.08936, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 6730.6094\n",
      "\n",
      "Epoch 00002: loss improved from 6736.08936 to 6730.60938, saving model to ./chkpt1\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 47ms/step - loss: 6694.9629\n",
      "\n",
      "Epoch 00003: loss improved from 6730.60938 to 6694.96289, saving model to ./chkpt1\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 43ms/step - loss: 6556.9419\n",
      "\n",
      "Epoch 00004: loss improved from 6694.96289 to 6556.94189, saving model to ./chkpt1\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 43ms/step - loss: 6240.3755\n",
      "\n",
      "Epoch 00005: loss improved from 6556.94189 to 6240.37549, saving model to ./chkpt1\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 42ms/step - loss: 5774.2710\n",
      "\n",
      "Epoch 00006: loss improved from 6240.37549 to 5774.27100, saving model to ./chkpt1\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 5272.2627\n",
      "\n",
      "Epoch 00007: loss improved from 5774.27100 to 5272.26270, saving model to ./chkpt1\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 4811.1758\n",
      "\n",
      "Epoch 00008: loss improved from 5272.26270 to 4811.17578, saving model to ./chkpt1\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 4411.6484\n",
      "\n",
      "Epoch 00009: loss improved from 4811.17578 to 4411.64844, saving model to ./chkpt1\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 4070.0745\n",
      "\n",
      "Epoch 00010: loss improved from 4411.64844 to 4070.07446, saving model to ./chkpt1\n",
      "Time taken: 25.957428216934204\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./distmult_logs')\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'mean'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=30, \n",
    "                                     k=350,\n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=tf.keras.regularizers.L2(0.0001))\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint, tensorboard_callback])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "# the training can be visualised using the following command:\n",
    "# tensorboard --logdir='./distmult_logs' --port=8891 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 108s 522ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2602269005785717, 0.18793424014091398)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './small_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings can be visualised using the following command:\n",
    "# tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 316ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00023974997689174383, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters\n",
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       batch_size=3,\n",
    "                       use_filter={'train': dataset['train'],\n",
    "                                  'test': dataset['test']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "42/42 [==============================] - 102s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9626798121146883, 0.9292739015559253)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters (filters are file names instead of numpy arrays)\n",
    "# corruptions generated using entities subset\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=500,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1) # will give very high mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 5s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10250035472836136, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o')\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 107s 521ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2602269005785717, 0.18793424014091398)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 108s 522ms/step\n",
      "Time taken: 120.94262504577637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2602269005785717, 0.18793424014091398)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above but just for sanity checking if entities_subset works or not\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=model.data_indexer.backend.get_all_entities(),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.897735  -1.8650669 -1.8186188 ...  7.3988943  7.3988943  7.887154 ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "# calibrate on the test set\n",
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.3826677  0.44528604 0.44815987 ... 0.89083356 0.89083356 0.9021644 ]\n",
      "[ 3834 18634  4066 ...  2021  9247 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.897735  -1.8650669 -1.8186188 ...  7.3988943  7.3988943  7.887154 ]\n",
      "[ 3834 18634  4066 ...  2021  9247 14612]\n"
     ]
    }
   ],
   "source": [
    "# check if the sorted probability indices match the sorted regular scores \n",
    "# It should be same as calibration doesnt change ranking, it just calibrates the range of scores\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 108s 523ms/step\n",
      "Time taken: 120.24336504936218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(335.3337410705548,\n",
       " 0.2602269005785717,\n",
       " 0.18793424014091398,\n",
       " 0.4024611018690674,\n",
       " 20438)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration should not affect the regular evaluation\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 108s 522ms/step\n",
      "Time taken: 120.79659652709961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(335.3337410705548,\n",
       " 0.2602269005785717,\n",
       " 0.18793424014091398,\n",
       " 0.4024611018690674,\n",
       " 20438)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loaded the model should return exact same results as earlier\n",
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=350, \n",
    "                                     scoring_type='DistMult')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[3.7724204  2.6032825  5.378133   ... 1.8153479  0.29437166 4.713559  ]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3834, 18634,  4066, ...,  2021,  9247, 14612]),\n",
       " array([ 3834, 18634,  4066, ...,  2021,  9247, 14612]),\n",
       " array([0.3826677 , 0.44528604, 0.44815987, ..., 0.89083356, 0.89083356,\n",
       "        0.9021644 ], dtype=float32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted calibration scores order and regular predict scores order must match\n",
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                 batch_size=10000)\n",
    "np.argsort(out), \\\n",
    "np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), \\\n",
    "np.sort(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 0.0Bytes, after: 12.897MB, consumed: 12.897MB; exec time: 9.9758s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 16s 57ms/step - loss: 1095.1136\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 1090.9705\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 1080.3376\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 1052.6915\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 1005.3367\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 945.7012\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 881.5021\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 818.8850\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 761.0177\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 15s 55ms/step - loss: 708.8445\n",
      "163.33203172683716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=10)\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 118s 2s/step\n",
      "Time taken: 118.36480283737183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1178.6854633525786, 0.07375494493608606, 0.0, 0.20351795674723555, 20438)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 120s 2s/step\n",
      "Time taken: 120.21392798423767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1178.6854633525786, 0.07375494493608606, 0.0, 0.20351795674723555, 20438)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 34.015MB, after: 46.787MB, consumed: 12.772MB; exec time: 124.36s\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 9643.4775\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 6s 196ms/step - loss: 9643.3984\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 6s 196ms/step - loss: 9643.2188\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 6s 197ms/step - loss: 9642.8271\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 6s 201ms/step - loss: 9641.9258\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 6s 195ms/step - loss: 9639.8584\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 6s 197ms/step - loss: 9635.4414\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 6s 198ms/step - loss: 9626.8184\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 6s 197ms/step - loss: 9611.4619\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 6s 197ms/step - loss: 9586.2705\n",
      "210.38515615463257\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs_split')\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[tensorboard_callback])\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:, 0]).union(set(dataset['train'][:, 2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 122s 2s/step\n",
      "Time taken: 122.74676465988159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1336.0252470887563, 0.06651532603609149, 0.0, 0.18416674821411097, 20438)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 341s 6s/step\n",
      "Time taken: 358.242479801178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1194.9367110284763,\n",
       " 0.1584834152025046,\n",
       " 0.10822976807906841,\n",
       " 0.2574126626871514,\n",
       " 20438)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model with partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 67.406MB, after: 80.176MB, consumed: 12.77MB; exec time: 124.95s\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 6s 190ms/step - loss: 9886.0283\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 6s 188ms/step - loss: 9885.5918\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 6s 188ms/step - loss: 9886.8545\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 6s 189ms/step - loss: 9886.6201\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 6s 189ms/step - loss: 9887.5215\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 6s 194ms/step - loss: 9886.1836\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 6s 198ms/step - loss: 9885.2510\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 6s 187ms/step - loss: 9885.3701\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 6s 187ms/step - loss: 9885.8809\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 6s 187ms/step - loss: 9885.8730\n",
      "208.36215567588806\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='Random')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[])\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 122s 2s/step\n",
      "Time taken: 123.16092610359192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7279.448282610823, 0.0005559095188126157, 0.0, 0.0004892846658185732, 20438)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 88ms/step - loss: 17412.8789\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 39ms/step - loss: 17409.4043\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 38ms/step - loss: 17388.6035\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 17307.3789\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 39ms/step - loss: 17095.1973\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 39ms/step - loss: 16681.0527\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 16038.2012\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 37ms/step - loss: 15221.4160\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 40ms/step - loss: 14335.6729\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 13471.0947\n",
      "Time taken: 24.852190494537354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['/m/0fvf9q', '/location/country/form_of_government', '/m/0k049']],\n",
       "       dtype=object),\n",
       " array([94.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=50,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 6s 128ms/step - loss: 17856.8340\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17844.8906\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17735.7676\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17341.3867\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 16681.2773\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 15790.5664\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 14691.8779\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 5s 108ms/step - loss: 13537.4414\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 12469.3301\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 5s 111ms/step - loss: 11533.4336\n",
      "(array([0, 1, 2, 3, 4, 5], dtype=int32), array([ 49,   1,  56,  54,  43, 110]))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n",
    "print(np.unique(clusters, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 15612.8779\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 15610.4873\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15607.6924\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15603.9346\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 15598.6699\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15591.2236\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15580.7832\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 15566.3682\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15546.8447\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15521.0059\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15487.4824\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15444.7588\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 15391.5029\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 15326.3516\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15248.0615\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15155.7275\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15048.4082\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 14926.0830\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 14788.2090\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 14635.9170\n",
      "        movie_name  year\n",
      "841         Tattah  2013\n",
      "840         Tattah  2013\n",
      "1677  Re-Generator  2010\n",
      "1676  Re-Generator  2010\n",
      "2477     Ambulance  2005\n",
      "2476     Ambulance  2005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 25ms/step - loss: 496.0767\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 495.4716\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 494.6113\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 493.0753\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 490.1469\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 484.6407\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 474.8754\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 458.7555\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 434.9001\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 407.1082\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 378.7333\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 351.8762\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 327.4932\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 305.6736\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 286.2159\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 268.8826\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 253.4297\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 239.6431\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 227.2515\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 216.0930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'The Vale'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Goodbrother of Hammerhorn'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Locke of Oldcastle'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Greyjoy of Pyke']],\n",
       "       dtype='<U44'),\n",
       " array([2.2265291 , 0.5242001 , 0.5207645 , 0.48392776, 0.46780267],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='DistMult')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
