{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 10:20:58.332400: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-06 10:20:58.956246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38238 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:39:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 62ms/step - loss: 6982.9214\n",
      "\n",
      "Epoch 1: loss improved from inf to 6982.92139, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 10:21:11.531385: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6986.3037\n",
      "\n",
      "Epoch 2: loss did not improve from 6982.92139\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6987.4985\n",
      "\n",
      "Epoch 3: loss did not improve from 6982.92139\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6987.1611\n",
      "\n",
      "Epoch 4: loss did not improve from 6982.92139\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6987.0215\n",
      "\n",
      "Epoch 5: loss did not improve from 6982.92139\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6986.9375\n",
      "\n",
      "Epoch 6: loss did not improve from 6982.92139\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6987.0557\n",
      "\n",
      "Epoch 7: loss did not improve from 6982.92139\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6987.1411\n",
      "\n",
      "Epoch 8: loss did not improve from 6982.92139\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 6986.9224\n",
      "\n",
      "Epoch 9: loss did not improve from 6982.92139\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 6986.8384\n",
      "\n",
      "Epoch 10: loss did not improve from 6982.92139\n",
      "Time taken: 21.169238805770874\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50,\n",
    "                                     scoring_type='Random')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=25,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 413ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005052450328641726"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 52ms/step - loss: 6736.0894\n",
      "\n",
      "Epoch 1: loss improved from inf to 6736.08936, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6730.6094\n",
      "\n",
      "Epoch 2: loss improved from 6736.08936 to 6730.60938, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6694.9629\n",
      "\n",
      "Epoch 3: loss improved from 6730.60938 to 6694.96289, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 6556.9419\n",
      "\n",
      "Epoch 4: loss improved from 6694.96289 to 6556.94189, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 6240.3755\n",
      "\n",
      "Epoch 5: loss improved from 6556.94189 to 6240.37549, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 5774.2710\n",
      "\n",
      "Epoch 6: loss improved from 6240.37549 to 5774.27100, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 5272.2627\n",
      "\n",
      "Epoch 7: loss improved from 5774.27100 to 5272.26270, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 35ms/step - loss: 4811.1758\n",
      "\n",
      "Epoch 8: loss improved from 5272.26270 to 4811.17578, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 4411.6484\n",
      "\n",
      "Epoch 9: loss improved from 4811.17578 to 4411.64844, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 4070.0745\n",
      "\n",
      "Epoch 10: loss improved from 4411.64844 to 4070.07446, saving model to ./chkpt1\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Time taken: 26.048795223236084\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./distmult_logs')\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'mean'})\n",
    "loss = NLLMulticlass({'reduction': 'mean'})\n",
    "model = ScoringBasedEmbeddingModel(eta=30, \n",
    "                                     k=350,\n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=tf.keras.regularizers.L2(0.0001))\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint, tensorboard_callback])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "# the training can be visualised using the following command:\n",
    "# tensorboard --logdir='./distmult_logs' --port=8891 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 415ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2602270358415026, 0.18793424014091398)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './small_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings can be visualised using the following command:\n",
    "# tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 326ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00023974997689174383, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters\n",
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       batch_size=3,\n",
    "                       use_filter={'train': dataset['train'],\n",
    "                                  'test': dataset['test']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "42/42 [==============================] - 80s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9626798121146883, 0.9292739015559253)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using filters (filters are file names instead of numpy arrays)\n",
    "# corruptions generated using entities subset\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=500,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1) # will give very high mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "      8/Unknown - 0s 51ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 09:44:30.420311: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/assert_equal_1/Assert/AssertGuard/branch_executed/_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10250035630223059, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o')\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 413ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2602270358415026, 0.18793424014091398)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full evaluation (default protocol) using filters\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 412ms/step\n",
      "Time taken: 95.00184178352356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2602270358415026, 0.18793424014091398)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above but just for sanity checking if entities_subset works or not\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=model.data_indexer.backend.get_all_entities(),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mrr_score(ranks), hits_at_n_score(ranks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.897735  -1.8650666 -1.8186185 ...  7.398894   7.3988943  7.887154 ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "# calibrate on the test set\n",
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.3826677  0.44528607 0.44815987 ... 0.89083356 0.89083356 0.9021644 ]\n",
      "[ 3834 18634  4066 ...  2021  9247 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.897735  -1.8650666 -1.8186185 ...  7.398894   7.3988943  7.887154 ]\n",
      "[ 3834 18634  4066 ...  9247  2021 14612]\n"
     ]
    }
   ],
   "source": [
    "# check if the sorted probability indices match the sorted regular scores \n",
    "# It should be same as calibration doesnt change ranking, it just calibrates the range of scores\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "410/410 [==============================] - 175s 427ms/step\n",
      "Time taken: 185.4112503528595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(335.33369214208824,\n",
       " 0.26022703584150253,\n",
       " 0.18793424014091398,\n",
       " 0.4024611018690674,\n",
       " 40876)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calibration should not affect the regular evaluation\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 85s 412ms/step\n",
      "Time taken: 94.97969150543213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(335.33369214208824,\n",
       " 0.2602270358415026,\n",
       " 0.18793424014091398,\n",
       " 0.4024611018690674,\n",
       " 20438)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loaded the model should return exact same results as earlier\n",
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=350, \n",
    "                                     scoring_type='DistMult')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[3.7724204  2.6032825  5.378133   ... 1.8153479  0.29437166 4.713559  ]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3834, 18634,  4066, ...,  2021,  9247, 14612]),\n",
       " array([ 3834, 18634,  4066, ...,  2021,  9247, 14612]),\n",
       " array([0.3826677 , 0.44528604, 0.44815987, ..., 0.89083356, 0.89083356,\n",
       "        0.9021644 ], dtype=float32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted calibration scores order and regular predict scores order must match\n",
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                 batch_size=10000)\n",
    "np.argsort(out), \\\n",
    "np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), \\\n",
    "np.sort(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 0.0Bytes, after: 12.81MB, consumed: 12.81MB; exec time: 20.264s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:48:59.955534: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 14:49:00.580059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38238 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:39:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 26s 96ms/step - loss: 1095.1127\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 26s 93ms/step - loss: 1090.9618\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 26s 96ms/step - loss: 1080.2867\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 1052.5120\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 26s 93ms/step - loss: 1004.9535\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 25s 93ms/step - loss: 945.0866\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 880.6619\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 27s 97ms/step - loss: 817.8425\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 26s 95ms/step - loss: 759.7907\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 26s 94ms/step - loss: 707.4430\n",
      "281.769056558609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=10)\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 227s 4s/step\n",
      "Time taken: 227.30635476112366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1171.3525540659555, 0.07391564339336283, 0.0, 0.20427634797925434, 20438)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 226s 4s/step\n",
      "Time taken: 225.89381051063538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1171.3525540659555, 0.07391564339336283, 0.0, 0.20427634797925434, 20438)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='DistMult')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 31.779MB, after: 44.602MB, consumed: 12.823MB; exec time: 150.55s\n",
      "Epoch 1/10\n",
      "     10/Unknown - 1s 112ms/step - loss: 9905.5361 2 2\n",
      "     12/Unknown - 3s 261ms/step - loss: 10093.00002 2\n",
      "     13/Unknown - 4s 337ms/step - loss: 10167.41502 2\n",
      "     23/Unknown - 6s 258ms/step - loss: 9672.1377 2 2\n",
      "     28/Unknown - 7s 261ms/step - loss: 9846.09282 2\n",
      "     31/Unknown - 9s 279ms/step - loss: 9960.09382 2\n",
      "32/32 [==============================] - 9s 292ms/step - loss: 9643.4775\n",
      "Epoch 2/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9702.43952 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9733.74512 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9789.68852 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9655.31642 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9737.72852 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9799.10842 2\n",
      "32/32 [==============================] - 8s 257ms/step - loss: 9643.3975\n",
      "Epoch 3/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9676.61912 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9694.80372 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9728.36132 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9650.80182 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9704.71882 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9746.45902 2\n",
      "32/32 [==============================] - 8s 263ms/step - loss: 9643.2178\n",
      "Epoch 4/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9666.33592 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9679.14452 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9703.08692 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9648.49222 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9688.49322 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9720.04492 2\n",
      "32/32 [==============================] - 8s 265ms/step - loss: 9642.8262\n",
      "Epoch 5/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9660.50682 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9670.38572 2\n",
      "13/32 [===========>..................] - ETA: 5s - loss: 9688.98442 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9646.65822 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9678.32712 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9703.59382 2\n",
      "32/32 [==============================] - 9s 268ms/step - loss: 9641.9248\n",
      "Epoch 6/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9656.08792 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9664.11332 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9679.28912 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9644.31932 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9670.29002 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9691.18072 2\n",
      "32/32 [==============================] - 8s 262ms/step - loss: 9639.8564\n",
      "Epoch 7/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9651.30862 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9658.02732 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9670.78612 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9640.27442 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9661.87502 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9679.37502 2\n",
      "32/32 [==============================] - 8s 262ms/step - loss: 9635.4365\n",
      "Epoch 8/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9644.34862 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9650.05082 2\n",
      "13/32 [===========>..................] - ETA: 5s - loss: 9660.94432 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9632.76462 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9650.60162 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9665.19142 2\n",
      "32/32 [==============================] - 8s 265ms/step - loss: 9626.8018\n",
      "Epoch 9/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9632.88092 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9637.70702 2\n",
      "13/32 [===========>..................] - ETA: 4s - loss: 9647.02442 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9619.36722 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9633.63282 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9645.48342 2\n",
      "32/32 [==============================] - 8s 262ms/step - loss: 9611.4277\n",
      "Epoch 10/10\n",
      "10/32 [========>.....................] - ETA: 1s - loss: 9614.05862 2\n",
      "11/32 [=========>....................] - ETA: 4s - loss: 9618.05472 2\n",
      "13/32 [===========>..................] - ETA: 5s - loss: 9625.91802 2\n",
      "23/32 [====================>.........] - ETA: 1s - loss: 9597.08592 2\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 9607.67772 2\n",
      "31/32 [============================>.] - ETA: 0s - loss: 9616.76862 2\n",
      "32/32 [==============================] - 8s 264ms/step - loss: 9586.2090\n",
      "267.40865206718445\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs_split')\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[tensorboard_callback])\n",
    "print((time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:, 0]).union(set(dataset['train'][:, 2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 230s 4s/step\n",
      "Time taken: 230.9751660823822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1335.5136999706428, 0.06652368353952463, 0.0, 0.1842646051472747, 20438)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 431s 8s/step\n",
      "Time taken: 451.3540623188019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1194.4257021234955,\n",
       " 0.15864779365563283,\n",
       " 0.10859673157843233,\n",
       " 0.2576817692533516,\n",
       " 20438)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model with partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                     k=50, \n",
    "                                     scoring_type='Random')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=10000, use_partitioning=True,\n",
    "                     epochs=10, callbacks=[])\n",
    "print((time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 88ms/step - loss: 17412.8789\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 39ms/step - loss: 17409.4043\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 38ms/step - loss: 17388.6035\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 17307.3789\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 39ms/step - loss: 17095.1973\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 39ms/step - loss: 16681.0527\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 16038.2012\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 37ms/step - loss: 15221.4160\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 40ms/step - loss: 14335.6729\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 13471.0947\n",
      "Time taken: 24.852190494537354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['/m/0fvf9q', '/location/country/form_of_government', '/m/0k049']],\n",
       "       dtype=object),\n",
       " array([94.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=50,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 6s 128ms/step - loss: 17856.8340\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17844.8906\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17735.7676\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 17341.3867\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 16681.2773\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 15790.5664\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 14691.8779\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 5s 108ms/step - loss: 13537.4414\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 12469.3301\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 5s 111ms/step - loss: 11533.4336\n",
      "(array([0, 1, 2, 3, 4, 5], dtype=int32), array([ 49,   1,  56,  54,  43, 110]))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n",
    "print(np.unique(clusters, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 15612.8779\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 15610.4873\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15607.6924\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15603.9346\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 15598.6699\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15591.2236\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15580.7832\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 15566.3682\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15546.8447\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15521.0059\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 15487.4824\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15444.7588\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 15391.5029\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 15326.3516\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15248.0615\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 15155.7275\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 15048.4082\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 14926.0830\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 14788.2090\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 14635.9170\n",
      "        movie_name  year\n",
      "841         Tattah  2013\n",
      "840         Tattah  2013\n",
      "1677  Re-Generator  2010\n",
      "1676  Re-Generator  2010\n",
      "2477     Ambulance  2005\n",
      "2476     Ambulance  2005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 25ms/step - loss: 496.0767\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 495.4716\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 494.6113\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 493.0753\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 490.1469\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 484.6407\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 474.8754\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 458.7555\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 434.9001\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 407.1082\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 378.7333\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 351.8762\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 327.4932\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 305.6736\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 286.2159\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 268.8826\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 253.4297\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 239.6431\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 227.2515\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 216.0930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'The Vale'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Goodbrother of Hammerhorn'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Locke of Oldcastle'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Greyjoy of Pyke']],\n",
       "       dtype='<U44'),\n",
       " array([2.2265291 , 0.5242001 , 0.5207645 , 0.48392776, 0.46780267],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='DistMult')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.8",
   "language": "python",
   "name": "tf2.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
