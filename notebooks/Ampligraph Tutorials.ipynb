{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-dev20201208\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.5'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 65ms/step - loss: 16389.1543\n",
      "\n",
      "Epoch 00001: loss improved from inf to 16389.15430, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 10464.8750\n",
      "\n",
      "Epoch 00002: loss improved from 16389.15430 to 10464.87500, saving model to ./chkpt1\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 7459.9829\n",
      "\n",
      "Epoch 00003: loss improved from 10464.87500 to 7459.98291, saving model to ./chkpt1\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 5856.6294\n",
      "\n",
      "Epoch 00004: loss improved from 7459.98291 to 5856.62939, saving model to ./chkpt1\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 4870.1465\n",
      "\n",
      "Epoch 00005: loss improved from 5856.62939 to 4870.14648, saving model to ./chkpt1\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 4196.1782\n",
      "\n",
      "Epoch 00006: loss improved from 4870.14648 to 4196.17822, saving model to ./chkpt1\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 3709.9500\n",
      "\n",
      "Epoch 00007: loss improved from 4196.17822 to 3709.94995, saving model to ./chkpt1\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 3341.0066\n",
      "\n",
      "Epoch 00008: loss improved from 3709.94995 to 3341.00659, saving model to ./chkpt1\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 3052.2170\n",
      "\n",
      "Epoch 00009: loss improved from 3341.00659 to 3052.21704, saving model to ./chkpt1\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 2818.6941\n",
      "\n",
      "Epoch 00010: loss improved from 3052.21704 to 2818.69409, saving model to ./chkpt1\n",
      "Time taken: 8.68879246711731\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.01)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'sum'})\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 888ms/step\n"
     ]
    }
   ],
   "source": [
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       use_filter={'train': dataset['train']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11594, 13650],\n",
       "       [ 5554,  6117],\n",
       "       [11965, 12250]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 54s 260ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.964608816909678"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks) # will give very high mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 67s 327ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22008795412679102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 70s 337ms/step\n",
      "Time taken: 69.58674621582031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22008795412679102"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=list(model.data_indexer.backend.entities_dict.values()),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.6117408 -2.5582652 -2.4347572 ... 30.218775  31.553251  41.351562 ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.41542763 0.41815612 0.42447674 ... 0.9985715  0.9989201  0.9998617 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.6117408 -2.5582652 -2.4347572 ... 30.218775  31.553251  41.351562 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 68s 328ms/step\n",
      "Time taken: 67.6325569152832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.09800371856346,\n",
       " 0.22008795412679102,\n",
       " 0.13516488893238085,\n",
       " 0.3928711224190234,\n",
       " 20438)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 67s 324ms/step\n",
      "Time taken: 66.87737703323364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.09800371856346,\n",
       " 0.22008795412679102,\n",
       " 0.13516488893238085,\n",
       " 0.3928711224190234,\n",
       " 20438)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='ComplEx')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[14.611031    9.369465    7.550661   ...  5.5516806   0.93437165\n",
      "  5.4705043 ]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([0.41542763, 0.41815612, 0.42447674, ..., 0.9985715 , 0.9989201 ,\n",
       "        0.9998617 ], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "np.argsort(out), np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), np.sort(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 896.0Bytes, after: 12.748MB, consumed: 12.747MB; exec time: 31.495s\n"
     ]
    }
   ],
   "source": [
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 1587.2365\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 1413.6970\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 1251.7213\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 1116.2098\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 1007.5591\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 19s 69ms/step - loss: 920.2757\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 19s 70ms/step - loss: 849.2607\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 790.6967\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 19s 70ms/step - loss: 741.4282\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 699.5930\n",
      "204.38951015472412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 130s 2s/step\n",
      "Time taken: 130.39389443397522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(464.99716214893823, 0.09894986543567783, 0.0, 0.2728251296604364, 20438)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=DummyBackend,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 131s 2s/step\n",
      "Time taken: 130.80136060714722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(464.99716214893823, 0.09894986543567783, 0.0, 0.2728251296604364, 20438)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 102.46MB, after: 115.18MB, consumed: 12.722MB; exec time: 96.584s\n",
      "Epoch 1/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1765.2789\n",
      "Epoch 2/10\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1750.7335\n",
      "Epoch 3/10\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 1739.2776\n",
      "Epoch 4/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1728.5615\n",
      "Epoch 5/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1718.0000\n",
      "Epoch 6/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1707.3138\n",
      "Epoch 7/10\n",
      "277/277 [==============================] - 12s 43ms/step - loss: 1696.3335\n",
      "Epoch 8/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1685.0465\n",
      "Epoch 9/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1673.4987\n",
      "Epoch 10/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1661.8308\n",
      "227.05004739761353\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.0001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=1000, use_partitioning=True,\n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14505"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['train'][:, 0]).union(set(dataset['train'][:, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 133s 3s/step\n",
      "Time taken: 133.19726157188416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1852.678613367257, 0.08854410426360261, 0.0, 0.2376944906546629, 20438)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 333s 6s/step\n",
      "Time taken: 333.0659215450287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1724.8188912809474,\n",
       " 0.1961136583183794,\n",
       " 0.1436539778843331,\n",
       " 0.3027448869752422,\n",
       " 20438)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 64ms/step - loss: 17412.5391\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 17398.1309\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 17296.1465\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 16878.4785\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 15909.9961\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 14546.6768\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 13176.4453\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 11976.7051\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 10962.2529\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 10104.8604\n",
      "Time taken: 8.017615556716919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['/m/0fvf9q', '/location/country/form_of_government', '/m/0m313']],\n",
       "       dtype=object),\n",
       " array([36.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 17856.8047\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 17844.0449\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 17730.1875: 0s - loss: 17800.8 - ETA: 0s - loss: 17774.0\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 17330.3867\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 16672.2637\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 15780.3242\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 14677.3369\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 13518.6426\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 12447.1543\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 11510.4658\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 15612.8799\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 15610.5010\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 15607.7412\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15604.0674\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 15598.9365\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 15591.7188\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15581.6055\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 15567.6807\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15548.8184\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 15523.8721\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 15491.4668\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 15450.2480\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15398.7607\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15335.4668\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 15259.4141\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 15169.6143\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 15065.2783\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 14945.5713\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 14810.6758\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 14660.7256\n",
      "           movie_name  year\n",
      "2332          Karaula  2006\n",
      "2331          Karaula  2006\n",
      "4250          Deewana  1992\n",
      "4251          Deewana  1992\n",
      "5100  Naked Vengeance  1985\n",
      "5101  Naked Vengeance  1985\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 491.7411\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 486.8790\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 483.2306\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 480.0301\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 477.2192\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 474.6082\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 472.1904\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 469.9152\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 467.8103\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 465.7492\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 463.8291\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 461.9291\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 460.1179\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 458.3310\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 456.6570\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 455.0156\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 453.3794\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 451.8036\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 450.2982\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 448.8355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Frey of the Crossing'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Stark of Winterfell'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH',\n",
       "         \"House Targaryen of King's Landing\"],\n",
       "        ['Eddard Stark', 'ALLIED_WITH',\n",
       "         'House Lannister of Casterly Rock'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Greyjoy of Pyke']],\n",
       "       dtype='<U44'),\n",
       " array([-1.0268682, -1.0519509, -1.0546845, -1.1503534, -1.1699506],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adagrad', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_nightly",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
