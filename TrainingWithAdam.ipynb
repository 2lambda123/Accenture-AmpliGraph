{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.3'))\n",
    "\n",
    "\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "full_dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would go into separate classes, like we had in ampligraph 1 (for loss functions and initializers)\n",
    "# initializer\n",
    "def xavier(in_shape, out_shape):\n",
    "    std = np.sqrt(2 / (in_shape + out_shape))\n",
    "    return np.random.normal(0, std, size=(in_shape, out_shape)).astype(np.float32)\n",
    "\n",
    "#loss function\n",
    "def nll(scores_pred, eta):\n",
    "    scores_neg = scores_pred[1]\n",
    "    scores_pos = scores_pred[0]\n",
    "\n",
    "    scores_neg_reshaped = tf.reshape(scores_neg, [eta, tf.shape(scores_pos)[0]])\n",
    "    neg_exp = tf.exp(scores_neg_reshaped)\n",
    "    pos_exp = tf.exp(scores_pos)\n",
    "    softmax_score = pos_exp / (tf.reduce_sum(neg_exp, axis=0) + pos_exp)\n",
    "\n",
    "    loss = -tf.reduce_sum(tf.math.log(softmax_score))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_to_idx, ent_to_idx = create_mappings(full_dataset['train'])\n",
    "train_dataset = to_idx(full_dataset['train'], rel_to_idx=rel_to_idx, ent_to_idx=ent_to_idx)\n",
    "\n",
    "pd.DataFrame(train_dataset, columns=['s', 'p', 'o']).to_csv('fb15k_237_train.csv', \n",
    "                                                           header=False,\n",
    "                                                           index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = to_idx(full_dataset['test'], rel_to_idx=rel_to_idx, ent_to_idx=ent_to_idx)\n",
    "\n",
    "pd.DataFrame(test_dataset, columns=['s', 'p', 'o']).to_csv('fb15k_237_test.csv', \n",
    "                                                           header=False,\n",
    "                                                           index=False, sep='\\t')\n",
    "\n",
    "\n",
    "x_filter = np.concatenate([full_dataset['train'], full_dataset['valid'], full_dataset['test']], 0)\n",
    "\n",
    "filter_dataset = to_idx(x_filter, rel_to_idx=rel_to_idx, ent_to_idx=ent_to_idx)\n",
    "\n",
    "pd.DataFrame(filter_dataset, columns=['s', 'p', 'o']).to_csv('fb15k_237_filter.csv', \n",
    "                                                           header=False,\n",
    "                                                           index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                                     k=300, \n",
    "                                                     max_ent_size=14505, \n",
    "                                                     max_rel_size=237,\n",
    "                                                     scoring_type='ComplEx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you have already saved the checkpoint\n",
    "# model.load_weights('./chkpt1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "     10/Unknown - 1s 124ms/step - loss: 53752.6797\n",
      "Epoch 00001: loss improved from inf to 53752.67969, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 53752.6797\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53745.4141\n",
      "Epoch 00002: loss improved from 53752.67969 to 53745.41406, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 53745.4141\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53732.8398\n",
      "Epoch 00003: loss improved from 53745.41406 to 53732.83984, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 53732.8398\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53706.2305\n",
      "Epoch 00004: loss improved from 53732.83984 to 53706.23047, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 53706.2305\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53650.6328\n",
      "Epoch 00005: loss improved from 53706.23047 to 53650.63281, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 53650.6328\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53542.5352\n",
      "Epoch 00006: loss improved from 53650.63281 to 53542.53516, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 53542.5352\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53348.9336\n",
      "Epoch 00007: loss improved from 53542.53516 to 53348.93359, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 53348.9336\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 53028.2344\n",
      "Epoch 00008: loss improved from 53348.93359 to 53028.23438, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 53028.2344\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 52533.3008\n",
      "Epoch 00009: loss improved from 53028.23438 to 52533.30078, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 52533.3008\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 51818.2773\n",
      "Epoch 00010: loss improved from 52533.30078 to 51818.27734, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 51818.2773\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 50848.5898\n",
      "Epoch 00011: loss improved from 51818.27734 to 50848.58984, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 50848.5898\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 49615.4805\n",
      "Epoch 00012: loss improved from 50848.58984 to 49615.48047, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 49615.4805\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 48145.0898\n",
      "Epoch 00013: loss improved from 49615.48047 to 48145.08984, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 48145.0898\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 46497.4453\n",
      "Epoch 00014: loss improved from 48145.08984 to 46497.44531, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 46497.4453\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 44755.0000\n",
      "Epoch 00015: loss improved from 46497.44531 to 44755.00000, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 44755.0000\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 42992.7617\n",
      "Epoch 00016: loss improved from 44755.00000 to 42992.76172, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 42992.7617\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 41268.4102\n",
      "Epoch 00017: loss improved from 42992.76172 to 41268.41016, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 41268.4102\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 39620.3047\n",
      "Epoch 00018: loss improved from 41268.41016 to 39620.30469, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 39620.3047\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 38062.4258\n",
      "Epoch 00019: loss improved from 39620.30469 to 38062.42578, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 38062.4258\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 36602.7500\n",
      "Epoch 00020: loss improved from 38062.42578 to 36602.75000, saving model to ./chkpt1\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 36602.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd6d7a63d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit('/home/spai/code/ampligraph_projects/tf2/AmpliGraph-Lab/fb15k_237_train.csv',\n",
    "         batch_size=30000,\n",
    "         epochs=20, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"scoring_based_embedding_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ComplEx (ComplEx)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "corruption_generation_layer_ multiple                  0         \n",
      "_________________________________________________________________\n",
      "embedding_lookup_layer (Embe multiple                  8845200   \n",
      "_________________________________________________________________\n",
      "loss (Mean)                  multiple                  2         \n",
      "=================================================================\n",
      "Total params: 8,845,202\n",
      "Trainable params: 8,845,200\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639/639 [==============================] - 12s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/tf2/AmpliGraph-Lab/fb15k_237_test.csv')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "import tensorflow as tf\n",
    "class Test():\n",
    "    def __init__(self):\n",
    "\n",
    "        \n",
    "        self.data_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/tf2/AmpliGraph-Lab/fb15k_237_test.csv', \n",
    "                                      backend=SQLiteAdapter,\n",
    "                                      batch_size=100, dataset_type=\"test\", verbose=True) \n",
    "\n",
    "        num_ents = self.data_loader.backend.mapper.ents_length\n",
    "        num_rels = self.data_loader.backend.mapper.rels_length\n",
    "        max_ent_size = self.data_loader.backend.mapper.ents_length\n",
    "\n",
    "\n",
    "        dataset = self.data_loader.get_batch()\n",
    "        \n",
    "        print('iterating...', dataset)\n",
    "        total_triples = 0\n",
    "    #with self.data_loader.backend:\n",
    "        for inputs in dataset:\n",
    "            total_triples += len(inputs)\n",
    "\n",
    "        print(total_triples)\n",
    "            \n",
    "Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is responsible for training the embeddings on multiple GPUs\n",
    "class ModelTrainer():\n",
    "        \n",
    "    def __init__(self, batch_size, max_ents, max_rels, k, dataset, seed=0):\n",
    "        # max_ent_size - is the max embeddings that can be loaded in memory\n",
    "        np.random.seed(seed)\n",
    "        self.eta = 10\n",
    "        self.models = None # would be a list later (one per device)\n",
    "        self.optimizers = None # would be a list later (one per device)\n",
    "        \n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.num_ents = max_ents\n",
    "        self.num_rels = max_rels\n",
    "        self.max_ent_size = max_ents\n",
    "        \n",
    "        # create the dataset handle\n",
    "        self.data_loader = GraphDataLoader(dataset, \n",
    "                              batch_size=batch_size, dataset_type=\"train\")\n",
    "        # create the model and optimizer\n",
    "        with tf.device('GPU:0'):\n",
    "            self.optimizers = tf.optimizers.Adam(lr=0.001)\n",
    "            self.models = ScoringBasedEmbeddingModel(eta=self.eta, \n",
    "                                                     k=k, \n",
    "                                                     max_ent_size=self.max_ent_size, \n",
    "                                                     max_rel_size=self.num_rels,\n",
    "                                                     scoring_type='ComplEx')\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.k = self.models.k\n",
    "        \n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, inputs, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get the model predictions\n",
    "            preds = self.models(inputs, training=0)\n",
    "            # compute the loss\n",
    "            loss = nll(preds, self.eta)\n",
    "            # regularizer - will be in a separate class like ampligraph 1\n",
    "            loss += (0.0001 * (tf.reduce_sum(tf.pow(tf.abs(self.models.encoding_layer.ent_emb), 3)) + \\\n",
    "                              tf.reduce_sum(tf.pow(tf.abs(self.models.encoding_layer.rel_emb), 3))))\n",
    "\n",
    "        # compute the grads\n",
    "        gradients = tape.gradient(loss, [self.models.encoding_layer.ent_emb, \n",
    "                                         self.models.encoding_layer.rel_emb])\n",
    "        # update the trainable params\n",
    "        optimizer.apply_gradients(zip(gradients, [self.models.encoding_layer.ent_emb, \n",
    "                                                  self.models.encoding_layer.rel_emb]))   \n",
    "        return loss\n",
    "        \n",
    "                \n",
    "\n",
    "    def train(self, epochs = 5):\n",
    "        # create the generator \n",
    "        dataset = tf.data.Dataset.from_generator(self.data_loader.get_batch,\n",
    "                                             output_types=(tf.int32),\n",
    "                                             output_shapes=((None, 3)))\n",
    "        dataset = dataset.prefetch(0)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(epochs):\n",
    "            total_loss = []\n",
    "            print(i)\n",
    "            self.global_epoch = i\n",
    "            # train on batches\n",
    "            for j, inputs in dataset.enumerate():\n",
    "                self.global_batch = 0\n",
    "                with tf.device('{}'.format('GPU:0')):\n",
    "                    loss = self.train_step(inputs, self.optimizers)\n",
    "                \n",
    "                total_loss.append(loss/inputs.shape[0])\n",
    "            \n",
    "            print('\\n\\n\\n\\nloss------------------{}:{}'.format(i, np.mean(total_loss)))\n",
    "        print('done')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_trainer = ModelTrainer(batch_size = 30000, \n",
    "                             max_ents = 14505, # ideally we should get from the dataset apis/partition manager\n",
    "                             max_rels=237,\n",
    "                             k=300,  \n",
    "                             dataset='/home/spai/code/ampligraph_projects/tf2/AmpliGraph-Lab/fb15k_237_train.csv')\n",
    "\n",
    "start = time.time()\n",
    "model_trainer.train(30)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator():\n",
    "    def __init__(self, ent_emb, rel_emb, models, max_ent_size, dataset, filterset):\n",
    "        self.ent_emb = ent_emb\n",
    "        self.rel_emb = rel_emb\n",
    "        self.models = models\n",
    "        self.max_ent_size = max_ent_size\n",
    "        self.dataset_handle = GraphDataLoader(dataset, \n",
    "                                  batch_size=100, dataset_type=\"test\")\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        for out in self.dataset_handle:\n",
    "\n",
    "            yield self.ent_emb[out[:, 0]], self.rel_emb[out[:, 1]], self.ent_emb[out[:, 2]]\n",
    "            \n",
    "    def evaluate(self):\n",
    "        dataset = tf.data.Dataset.from_generator(self.get_next_batch,\n",
    "                                                 output_types=(tf.float32, tf.float32, tf.float32),\n",
    "                                                 output_shapes=((None, self.models.k), \n",
    "                                                                (None, self.models.k), \n",
    "                                                                (None, self.models.k)))\n",
    "        dataset = dataset.prefetch(1)\n",
    "\n",
    "        self.all_ranks = []\n",
    "        for i, inputs in dataset.enumerate():\n",
    "            batch_size = self.max_ent_size\n",
    "            batch_count = np.int32(np.round(self.ent_emb.shape[0]/batch_size))\n",
    "            overall_rank = np.zeros((inputs[0].shape[0], 2))\n",
    "            for j in range(batch_count):\n",
    "                ent_embs = self.ent_emb[j * batch_size : (j+1) * batch_size, :]\n",
    "                rel_embs = self.rel_emb\n",
    "                with tf.device('GPU:0'):\n",
    "                    sub_rank, obj_rank = self.models.get_ranks(inputs, ent_embs)\n",
    "                overall_rank[:, 0] +=  sub_rank.numpy()\n",
    "                overall_rank[:, 1] +=  obj_rank.numpy()\n",
    "            overall_rank = overall_rank + 1\n",
    "            self.all_ranks.extend(overall_rank.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluator = ModelEvaluator(model_trainer.models.encoding_layer.ent_emb.numpy(),\n",
    "                                      model_trainer.models.encoding_layer.rel_emb.numpy(),\n",
    "                                      model_trainer.models,\n",
    "                                      max_ent_size=14505,\n",
    "                                      dataset='/home/spai/code/ampligraph_projects/tf2/AmpliGraph-Lab/fb15k_237_test.csv',\n",
    "                                      filterset='/home/spai/code/ampligraph_projects/tf2/AmpliGraph-Lab/fb15k_237_filter.csv')\n",
    "\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "model_evaluator.evaluate()\n",
    "print((time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_score(model_evaluator.all_ranks), hits_at_n_score(model_evaluator.all_ranks, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore this - partition related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8000),\n",
    "    tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8000)])\n",
    "\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "logical_gpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is responsible for training the embeddings on multiple GPUs\n",
    "class ModelTrainer():\n",
    "    \n",
    "    # this class manages the partitions\n",
    "    # This would later be responsible for persisting the input data and creating the partitions\n",
    "    # during training time, it will load the partitions and related embeddings \n",
    "    class PartitionManager():\n",
    "        def __init__(self, num_buckets, k, dataset, strategy, num_devices=1):\n",
    "            self.num_buckets = num_buckets\n",
    "            \n",
    "            num_entities = len(set(dataset[:, 0]).union(dataset[:, 2]))\n",
    "            num_rels = len(set(dataset[:, 1]))\n",
    "            \n",
    "            self.rel_to_idx, self.ent_to_idx = create_mappings(dataset)\n",
    "            dataset = to_idx(dataset, self.ent_to_idx, self.rel_to_idx)\n",
    "            \n",
    "            partitioner = PARTITION_ALGO_REGISTRY.get(strategy)(dataset, k=num_buckets)\n",
    "            self.partitions = partitioner.split()\n",
    "            \n",
    "            if num_buckets > 1:\n",
    "                self.num_partitions = np.sum(np.arange(1, num_buckets+1))\n",
    "            else:\n",
    "                self.num_partitions = num_buckets\n",
    "            \n",
    "            # needs to go into the database\n",
    "            self.entity_embeddings = xavier(num_entities, k)\n",
    "            self.rel_embeddings = xavier(num_rels, k)\n",
    "            \n",
    "            self.num_devices = 1\n",
    "            \n",
    "            # use multiple GPUs only if we do multiple partitions\n",
    "            if self.num_partitions > 1:\n",
    "                self.num_devices = num_devices\n",
    "            \n",
    "        \n",
    "        def get_next_partition(self):\n",
    "            # get the next partition to train on, along with the embeddings of nodes in that partition\n",
    "            for i in range(len(self.partitions)):\n",
    "                \n",
    "                    entities_of_partition = set(self.partitions[i][:, 0]).union(self.partitions[i][:, 2])\n",
    "                    partition_dict = dict(zip(entities_of_partition, \n",
    "                                              np.arange(len(entities_of_partition))))\n",
    "                    new_rel_dict = dict(zip(np.arange(len(self.rel_to_idx)), np.arange(len(self.rel_to_idx))))\n",
    "                    # remap the triples to reflect the position in the embedding matrix\n",
    "                    remapped_triples = to_idx(self.partitions[i], partition_dict, new_rel_dict)\n",
    "                    yield partition_dict, \\\n",
    "                            new_rel_dict, \\\n",
    "                            remapped_triples, \\\n",
    "                            self.entity_embeddings[list(partition_dict.keys()), :], \\\n",
    "                            self.rel_embeddings[list(new_rel_dict.keys()), :]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self, batch_size, max_ent_size, k, dataset, strategy, num_buckets=1, num_devices=1, seed=0):\n",
    "        # max_ent_size - is the max embeddings that can be loaded in memory\n",
    "        np.random.seed(seed)\n",
    "        self.eta = 10\n",
    "        self.models = None # would be a list later (one per device)\n",
    "        self.optimizers = None # would be a list later (one per device)\n",
    "        self.max_ent_size = max_ent_size\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.num_devices = num_devices\n",
    "        self.num_ents = len(set(dataset[:, 0]).union(dataset[:, 2]))\n",
    "        self.num_rels = len(set(dataset[:, 1]))\n",
    "        \n",
    "        # create embedding model and optimizer\n",
    "        for i in range(num_devices):\n",
    "            with tf.device('GPU:{}'.format(i)):\n",
    "                self.optimizers = tf.optimizers.Adam(lr=0.001)\n",
    "                self.models = ScoringBasedEmbeddingModel(eta=self.eta, \n",
    "                                                         k=k, \n",
    "                                                         max_ent_size=max_ent_size, \n",
    "                                                         max_rel_size=self.num_rels,\n",
    "                                                         scoring_type='ComplEx')\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.k = self.models.k\n",
    "        self.partition_manager = self.PartitionManager(num_buckets, self.k, dataset, strategy, num_devices)\n",
    "        \n",
    "        # this is the hyperparams of the optimizer - would be later moved \n",
    "        # to database calls and retrieved from partition manager\n",
    "        self.optimizer_hyperparams_ent = np.zeros(shape=(self.num_ents, 2, self.k), \n",
    "                                                  dtype=np.float32)\n",
    "        self.optimizer_hyperparams_rel = np.zeros(shape=(self.num_rels, 2, self.k), \n",
    "                                                  dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def train_dataset_generator(self, dataset):\n",
    "        # generator for the training data\n",
    "        batch_count = dataset.shape[0]//self.batch_size + 1\n",
    "        for j in range(batch_count):\n",
    "            inputs = dataset[j * self.batch_size : (j+1) * self.batch_size, :].astype(np.int32)\n",
    "            yield inputs\n",
    "            \n",
    "    def update_partion_embeddings_after_train(self):\n",
    "        # before changing the partition, save the trained embeddings and optimizer params\n",
    "        \n",
    "        self.partition_manager.entity_embeddings[list(self.ent_dict.keys()), :] = self.models.encoding_layer.ent_emb.numpy()[:len(self.ent_dict), :]\n",
    "        self.partition_manager.rel_embeddings[list(self.rel_dict.keys()), :] = self.models.encoding_layer.rel_emb.numpy()[:len(self.rel_dict), :]\n",
    "        \n",
    "        opt_weights = self.optimizers.get_weights()\n",
    "        if len(opt_weights)>0:\n",
    "            self.optimizer_hyperparams_rel[list(self.rel_dict.keys()), :, :] = np.concatenate([opt_weights[2][:len(self.rel_dict)][:, np.newaxis, :], \n",
    "                                                                                                 opt_weights[4][:len(self.rel_dict)][:, np.newaxis, :]], 1)\n",
    "            \n",
    "            self.optimizer_hyperparams_ent[list(self.ent_dict.keys()), :, :] = np.concatenate([opt_weights[1][:len(self.ent_dict)][:, np.newaxis, :], \n",
    "                                                                                             opt_weights[3][:len(self.ent_dict)][:, np.newaxis, :]], 1)\n",
    "        \n",
    "        \n",
    "    def change_partition(self):\n",
    "        # load a new partition and update the trainable params and optimizer hyperparams\n",
    "        self.ent_dict, self.rel_dict, remapped_triples, ent_embs, rel_embs = next(self.partition_iterator)\n",
    "        print('partition has {} triples', remapped_triples.shape)\n",
    "        self.partition_dataset_iterator = iter(self.train_dataset_generator(remapped_triples))\n",
    "        self.models.partition_change_updates(len(self.ent_dict), ent_embs, rel_embs)\n",
    "        if self.global_epoch >1:\n",
    "            # needs to be better handled\n",
    "            optimizer_rel_weights_updates_beta1 = self.optimizer_hyperparams_rel[list(self.rel_dict.keys()), 0, :]\n",
    "            optimizer_rel_weights_updates_beta2 = self.optimizer_hyperparams_rel[list(self.rel_dict.keys()), 1, :]\n",
    "            optimizer_ent_weights_updates_beta1 = self.optimizer_hyperparams_ent[list(self.ent_dict.keys()), 0, :]\n",
    "            optimizer_ent_weights_updates_beta2 = self.optimizer_hyperparams_ent[list(self.ent_dict.keys()), 1, :]\n",
    "            \n",
    "            optimizer_rel_weights_updates_beta1 = np.pad(optimizer_rel_weights_updates_beta1, \n",
    "                                                         ((0, self.num_rels - optimizer_rel_weights_updates_beta1.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            optimizer_rel_weights_updates_beta2 = np.pad(optimizer_rel_weights_updates_beta2, \n",
    "                                                         ((0, self.num_rels - optimizer_rel_weights_updates_beta2.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            optimizer_ent_weights_updates_beta1 = np.pad(optimizer_ent_weights_updates_beta1, \n",
    "                                                         ((0, self.max_ent_size - optimizer_ent_weights_updates_beta1.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            optimizer_ent_weights_updates_beta2 = np.pad(optimizer_ent_weights_updates_beta2, \n",
    "                                                         ((0, self.max_ent_size - optimizer_ent_weights_updates_beta2.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            \n",
    "            self.optimizers.set_weights(self.optimizers.get_weights())\n",
    "\n",
    "            self.optimizers.set_weights([self.optimizers.iterations.numpy(), \n",
    "                                         optimizer_ent_weights_updates_beta1,\n",
    "                                         optimizer_rel_weights_updates_beta1,\n",
    "                                         optimizer_ent_weights_updates_beta2,\n",
    "                                         optimizer_rel_weights_updates_beta2\n",
    "                                        ])\n",
    "\n",
    "            \n",
    "    def get_next_batch(self):\n",
    "        try:\n",
    "            self.partition_iterator = iter(self.partition_manager.get_next_partition())\n",
    "            # get new partition\n",
    "            self.change_partition()\n",
    "            while True:\n",
    "                try:\n",
    "                    # get batches from the current partition\n",
    "                    out = next(self.partition_dataset_iterator)\n",
    "                    yield out\n",
    "                except StopIteration:\n",
    "                    # if no more batch data - save the trained params and load next partition\n",
    "                    self.update_partion_embeddings_after_train()\n",
    "                    self.change_partition()\n",
    "        except StopIteration:\n",
    "            self.update_partion_embeddings_after_train()\n",
    "            #if no more partitions, end\n",
    "            return\n",
    "                \n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, inputs, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get the model predictions\n",
    "            preds = self.models(inputs, training=0)\n",
    "            # compute the loss\n",
    "            loss = nll(preds, self.eta)\n",
    "            # regularizer - will be in a separate class like ampligraph 1\n",
    "            loss += (0.0001 * (tf.reduce_sum(tf.pow(tf.abs(self.models.encoding_layer.ent_emb), 3)) + \\\n",
    "                              tf.reduce_sum(tf.pow(tf.abs(self.models.encoding_layer.rel_emb), 3))))\n",
    "\n",
    "        # compute the grads\n",
    "        gradients = tape.gradient(loss, [self.models.encoding_layer.ent_emb, \n",
    "                                         self.models.encoding_layer.rel_emb])\n",
    "        # update the trainable params\n",
    "        optimizer.apply_gradients(zip(gradients, [self.models.encoding_layer.ent_emb, \n",
    "                                                  self.models.encoding_layer.rel_emb]))   \n",
    "        return loss\n",
    "        \n",
    "                \n",
    "\n",
    "    def train(self, epochs = 5):\n",
    "        dataset = tf.data.Dataset.from_generator(self.get_next_batch,\n",
    "                                             output_types=(tf.int32),\n",
    "                                             output_shapes=((None, 3)))\n",
    "        dataset = dataset.prefetch(0)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(epochs):\n",
    "            total_loss = []\n",
    "            print(i)\n",
    "            self.global_epoch = i\n",
    "\n",
    "            for j, inputs in dataset.enumerate():\n",
    "                self.global_batch = 0\n",
    "                with tf.device('{}'.format('GPU:0')):\n",
    "                    loss = self.train_step(inputs, self.optimizers)\n",
    "                \n",
    "                total_loss.append(loss/inputs.shape[0])\n",
    "            \n",
    "            print('\\n\\n\\n\\nloss------------------{}:{}'.format(i, np.mean(total_loss)))\n",
    "        print('done')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets = 3\n",
    "model_trainer = ModelTrainer(30000, \n",
    "                             14505, \n",
    "                             k=300,\n",
    "                             dataset=full_dataset['train'], \n",
    "                             strategy='RandomVertices',\n",
    "                             num_buckets=num_buckets, \n",
    "                             num_devices=1)\n",
    "\n",
    "start = time.time()\n",
    "model_trainer.train(1)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training for 10 more epochs\n",
    "start = time.time()\n",
    "model_trainer.train(1)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare loss of prev model with model trained for 20 epochs.\n",
    "\n",
    "model_trainer = ModelTrainer(30000, \n",
    "                             14505, \n",
    "                             k=300,\n",
    "                             dataset=full_dataset['train'], \n",
    "                             strategy='Bucket',\n",
    "                             num_buckets=1, \n",
    "                             num_devices=1)\n",
    "\n",
    "start = time.time()\n",
    "model_trainer.train(2)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "from ampligraph.evaluation import filter_unseen_entities\n",
    "\n",
    "from ampligraph.datasets import NumpyDatasetAdapter\n",
    "filter_triples = np.concatenate([full_dataset['train'],\n",
    "                                full_dataset['valid'],\n",
    "                                full_dataset['test']], 0).astype(np.str)\n",
    "\n",
    "test_dataset = full_dataset['test']\n",
    "dataset_handle = NumpyDatasetAdapter()\n",
    "dataset_handle.use_mappings(model_trainer.partition_manager.rel_to_idx, \n",
    "                            model_trainer.partition_manager.ent_to_idx)\n",
    "\n",
    "# need to fix filter_unseen_entities - this is temp workaround\n",
    "model_trainer.models.ent_to_idx = model_trainer.partition_manager.ent_to_idx\n",
    "model_trainer.models.rel_to_idx = model_trainer.partition_manager.rel_to_idx\n",
    "test_dataset = filter_unseen_entities(test_dataset.astype(np.str), model_trainer.models, verbose=True)\n",
    "print(test_dataset.shape)\n",
    "dataset_handle.set_data(test_dataset, 'test')\n",
    "\n",
    "print(filter_triples.shape)\n",
    "filter_triples = filter_unseen_entities(filter_triples, model_trainer.models, verbose=True)\n",
    "dataset_handle.set_filter(filter_triples)\n",
    "\n",
    "print(filter_triples.shape)\n",
    "\n",
    "class ModelEvaluator():\n",
    "    def __init__(self, ent_emb, rel_emb, models, max_ent_size, dataset_handle):\n",
    "        self.ent_emb = ent_emb\n",
    "        self.rel_emb = rel_emb\n",
    "        self.models = models\n",
    "        self.max_ent_size = max_ent_size\n",
    "        self.dataset_handle = dataset_handle\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        batch_size = 100\n",
    "        batch_count = np.int32(np.round(self.dataset_handle.get_size('test')/batch_size))\n",
    "        test_generator = partial(self.dataset_handle.get_next_batch,\n",
    "                             dataset_type='test',\n",
    "                             use_filter=False, \n",
    "                             batches_count=batch_count)\n",
    "\n",
    "        batch_iterator = iter(test_generator())\n",
    "        print('Btahces: ', np.int32(np.round(batch_count/batch_size)))\n",
    "        for i in range(batch_count):\n",
    "            out = next(batch_iterator)\n",
    "\n",
    "            yield self.ent_emb[out[:, 0]], self.rel_emb[out[:, 1]], self.ent_emb[out[:, 2]]\n",
    "            \n",
    "    def evaluate(self):\n",
    "        dataset = tf.data.Dataset.from_generator(self.get_next_batch,\n",
    "                                                 output_types=(tf.float32, tf.float32, tf.float32),\n",
    "                                                 output_shapes=((None, self.models.k), \n",
    "                                                                (None, self.models.k), \n",
    "                                                                (None, self.models.k)))\n",
    "        dataset = dataset.prefetch(1)\n",
    "\n",
    "        self.all_ranks = []\n",
    "        for i, inputs in dataset.enumerate():\n",
    "            batch_size = self.max_ent_size\n",
    "            batch_count = np.int32(np.round(self.ent_emb.shape[0]/batch_size))\n",
    "            overall_rank = np.zeros((inputs[0].shape[0], 2))\n",
    "            for j in range(batch_count):\n",
    "                ent_embs = self.ent_emb[j * batch_size : (j+1) * batch_size, :]\n",
    "                rel_embs = self.rel_emb\n",
    "                with tf.device('GPU:0'):\n",
    "                    sub_rank, obj_rank = self.models.get_ranks(inputs, ent_embs)\n",
    "                overall_rank[:, 0] +=  sub_rank.numpy()\n",
    "                overall_rank[:, 1] +=  obj_rank.numpy()\n",
    "            overall_rank = overall_rank + 1\n",
    "            self.all_ranks.extend(overall_rank.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluator = ModelEvaluator(model_trainer.partition_manager.entity_embeddings,\n",
    "                                      model_trainer.partition_manager.rel_embeddings,\n",
    "                                      model_trainer.models,\n",
    "                                      max_ent_size=max_entities,\n",
    "                                      dataset_handle=dataset_handle)\n",
    "\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "model_evaluator.evaluate()\n",
    "print(i, (time.time()-start)/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluator.all_ranks = np.array(model_evaluator.all_ranks) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_score(model_evaluator.all_ranks), hits_at_n_score(model_evaluator.all_ranks, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_nightly",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
