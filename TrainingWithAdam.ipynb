{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-dev20201208\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.5'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to [Partitioned](#Training/eval-with-partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 2s 41ms/step - loss: 1.7917\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.7904\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 1.7790\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 1.7389\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 1.6729\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 1.5834\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.4727\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.3564\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 1.2489\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.1549\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 1.7918\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.7915\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7912\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.7908\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7902\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7893\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7882\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7866\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7844\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7816\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7778\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7731\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7672\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7599\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7512\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7409\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7289\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7152\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.6997\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.6825\n",
      "                movie_name  year\n",
      "1785           Interceptor  2009\n",
      "1786           Interceptor  2009\n",
      "1101  Last Flight to Abuja  2012\n",
      "1102  Last Flight to Abuja  2012\n",
      "2183   Showdown at Area 51  2007\n",
      "2184   Showdown at Area 51  2007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 20ms/step - loss: 4.9561\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.9071\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.8704\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.8381\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.8098\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.7835\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.7591\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.7362\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.7149\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.6942\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.6748\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.6557\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.6374\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.6194\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.6025\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.5860\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.5695\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.5536\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.5384\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.5237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Baelish of the Fingers'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'Larys Strong'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'Harlan Hunter'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Dalt of Lemonwood'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Tarbeck of Tarbeck Hall']],\n",
       "       dtype='<U44'),\n",
       " array([-1.3960195, -1.3976465, -1.4125105, -1.4159653, -1.4202976],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adagrad', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 45ms/step - loss: 1.6864\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.68641, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.0768\n",
      "\n",
      "Epoch 00002: loss improved from 1.68641 to 1.07681, saving model to ./chkpt1\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.7676\n",
      "\n",
      "Epoch 00003: loss improved from 1.07681 to 0.76761, saving model to ./chkpt1\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.6026\n",
      "\n",
      "Epoch 00004: loss improved from 0.76761 to 0.60263, saving model to ./chkpt1\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.5011\n",
      "\n",
      "Epoch 00005: loss improved from 0.60263 to 0.50113, saving model to ./chkpt1\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.4318\n",
      "\n",
      "Epoch 00006: loss improved from 0.50113 to 0.43178, saving model to ./chkpt1\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3817\n",
      "\n",
      "Epoch 00007: loss improved from 0.43178 to 0.38175, saving model to ./chkpt1\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.3438\n",
      "\n",
      "Epoch 00008: loss improved from 0.38175 to 0.34378, saving model to ./chkpt1\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 0.3141\n",
      "\n",
      "Epoch 00009: loss improved from 0.34378 to 0.31407, saving model to ./chkpt1\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 0.2900\n",
      "\n",
      "Epoch 00010: loss improved from 0.31407 to 0.29004, saving model to ./chkpt1\n",
      "Time taken: 7.73827338218689\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.01)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'sum'})\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import discover_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['/m/028_yv', '/location/country/form_of_government', '/m/0m313']],\n",
       "       dtype=object),\n",
       " array([66.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "272115 triples containing invalid keys skipped!\n",
      "2/2 [==============================] - 1s 410ms/step\n"
     ]
    }
   ],
   "source": [
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       use_filter={'train': dataset['train']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  128,     4],\n",
       "       [ 9137, 14157],\n",
       "       [11342, 14494]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 55s 265ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.964608816909678"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 68s 328ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16188943877767542"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 68s 331ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16188943877767542"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=list(model.data_indexer.entities_dict.values()),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.4154276  0.41815612 0.42447674 ... 0.9985715  0.9989201  0.9998617 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.611742  -2.5582657 -2.4347575 ... 30.218775  31.553257  41.35157  ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 59s 287ms/step\n",
      "Time taken: 59.169644355773926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.0981994324298,\n",
       " 0.22008795174458182,\n",
       " 0.13516488893238085,\n",
       " 0.3928711224190234,\n",
       " 20438)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ScoringBasedEmbeddingModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2dc8edb01f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m loaded_model = ScoringBasedEmbeddingModel(eta=5, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                      \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      scoring_type='ComplEx')\n\u001b[1;32m      5\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./calibrated_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ScoringBasedEmbeddingModel' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='ComplEx')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[14.611029   9.369468   7.55066   ...  5.55168    0.9343734  5.4705043]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([0.4154276 , 0.41815612, 0.42447674, ..., 0.9985715 , 0.9989201 ,\n",
       "        0.9998617 ], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "np.argsort(out), np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), np.sort(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 896.0Bytes, after: 12.98MB, consumed: 12.979MB; exec time: 31.859s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss=nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "274/274 [==============================] - 20s 72ms/step - loss: 1.5924\n",
      "Epoch 2/2\n",
      "274/274 [==============================] - 19s 68ms/step - loss: 1.4184\n",
      "51.306065797805786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=2)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 136s 3s/step\n",
      "Time taken: 136.147305727005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1664.7270770134064, 0.08627521188313407, 0.0, 0.23722967022213523, 20438)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=DummyBackend,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 131s 2s/step\n",
      "Time taken: 131.47749257087708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2613.1418925530875, 0.07781069124506454, 0.0, 0.22360309227908798, 20438)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 98.973MB, after: 109.88MB, consumed: 10.909MB; exec time: 88.649s\n",
      "Epoch 1/10\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 1.7248\n",
      "Epoch 2/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.6170\n",
      "Epoch 3/10\n",
      "277/277 [==============================] - 13s 45ms/step - loss: 1.5097\n",
      "Epoch 4/10\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 1.4106\n",
      "Epoch 5/10\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 1.3221\n",
      "Epoch 6/10\n",
      "277/277 [==============================] - 12s 45ms/step - loss: 1.2444\n",
      "Epoch 7/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.1767\n",
      "Epoch 8/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.1174\n",
      "Epoch 9/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.0654\n",
      "Epoch 10/10\n",
      "277/277 [==============================] - 12s 44ms/step - loss: 1.0194\n",
      "221.17552399635315\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss=nll)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=1000, use_partitioning=True,\n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 136s 3s/step\n",
      "Time taken: 135.5718469619751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(662.5322193952442, 0.08724452920413757, 0.0, 0.23862413151971817, 20438)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 297s 6s/step\n",
      "Time taken: 297.1006762981415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(664.6330365006361,\n",
       " 0.1936046545792393,\n",
       " 0.13240043057050593,\n",
       " 0.3117477248263039,\n",
       " 20438)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_nightly",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
