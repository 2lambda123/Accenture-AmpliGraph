{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-dev20201007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.4'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def nll(scores_pos, scores_neg):\n",
    "    neg_exp = tf.exp(scores_neg)\n",
    "    pos_exp = tf.exp(scores_pos)\n",
    "    softmax_score = pos_exp / (tf.reduce_sum(neg_exp, axis=0) + pos_exp)\n",
    "\n",
    "    loss = -tf.math.log(softmax_score)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "     28/Unknown - 2s 67ms/step - loss: 1.6903\n",
      "Epoch 00001: loss improved from inf to 1.68641, saving model to ./chkpt1\n",
      "29/29 [==============================] - 2s 68ms/step - loss: 1.6864\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.0768\n",
      "Epoch 00002: loss improved from 1.68641 to 1.07681, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 1.0768\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.7676\n",
      "Epoch 00003: loss improved from 1.07681 to 0.76761, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.7676\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.6026\n",
      "Epoch 00004: loss improved from 0.76761 to 0.60263, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.6026\n",
      "Epoch 5/10\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 0.5078\n",
      "9 triples containing invalid keys skipped!\n",
      "177/177 [==============================] - 9s 51ms/step\n",
      "\n",
      "Epoch 00005: loss improved from 0.60263 to 0.50113, saving model to ./chkpt1\n",
      "29/29 [==============================] - 10s 333ms/step - loss: 0.5011 - val_mrr: 0.3253 - val_mr: 431.8246 - val_hits@1: 0.2621 - val_hits@10: 0.4457\n",
      "Epoch 6/10\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 0.4365\n",
      "Epoch 00006: loss improved from 0.50113 to 0.43178, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.4318\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.3817\n",
      "Epoch 00007: loss improved from 0.43178 to 0.38175, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3817\n",
      "Epoch 8/10\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 0.3465\n",
      "Epoch 00008: loss improved from 0.38175 to 0.34378, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 23ms/step - loss: 0.3438\n",
      "Epoch 9/10\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 0.3162\n",
      "Epoch 00009: loss improved from 0.34378 to 0.31407, saving model to ./chkpt1\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 0.3141\n",
      "Epoch 10/10\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 0.2918\n",
      "9 triples containing invalid keys skipped!\n",
      "177/177 [==============================] - 8s 45ms/step\n",
      "\n",
      "Epoch 00010: loss improved from 0.31407 to 0.29004, saving model to ./chkpt1\n",
      "29/29 [==============================] - 9s 300ms/step - loss: 0.2900 - val_mrr: 0.2796 - val_mr: 488.4926 - val_hits@1: 0.2099 - val_hits@10: 0.4129\n",
      "Time taken: 25.837014198303223\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.01)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'sum'})\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "start = time.time()\n",
    "model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=5,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = '/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.6117415 -2.5582657 -2.4347568 ... 30.218767  31.553259  41.35157  ]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.44457036 0.44533417 0.44709924 ... 0.8423783  0.8523569  0.9105091 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.6117415 -2.5582657 -2.4347568 ... 30.218767  31.553259  41.35157  ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n"
     ]
    }
   ],
   "source": [
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 52s 254ms/step\n",
      "Time taken: 52.35202598571777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(311.1254525883159,\n",
       " 0.3877527518637535,\n",
       " 0.3215334181426754,\n",
       " 0.5160485370388492,\n",
       " 20438)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 52s 254ms/step\n",
      "Time taken: 52.437538146972656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(311.1254525883159,\n",
       " 0.3877527518637535,\n",
       " 0.3215334181426754,\n",
       " 0.5160485370388492,\n",
       " 20438)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='ComplEx')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[14.611031   9.369465   7.550661  ...  5.55168    0.9343749  5.470505 ]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([0.44457036, 0.44533417, 0.44709924, ..., 0.8423783 , 0.8523569 ,\n",
       "        0.9105091 ], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "np.argsort(out), np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), np.sort(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 0.0Bytes, after: 12.886MB, consumed: 12.886MB; exec time: 31.461s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss=nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "274/274 [==============================] - 16s 58ms/step - loss: 1.5947\n",
      "28.386675596237183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,\n",
    "                     epochs=2)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 129s 2s/step\n",
      "Time taken: 129.33987951278687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2613.1418925530875, 0.07781069124506454, 0.0, 0.22360309227908798, 20438)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=DummyBackend,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 131s 2s/step\n",
      "Time taken: 131.47749257087708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2613.1418925530875, 0.07781069124506454, 0.0, 0.22360309227908798, 20438)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 102.66MB, after: 115.36MB, consumed: 12.705MB; exec time: 86.93s\n",
      "Epoch 2/10\n",
      "277/277 [==============================] - 9s 34ms/step - loss: 1.7309\n",
      "Epoch 3/10\n",
      "277/277 [==============================] - 9s 32ms/step - loss: 1.6420\n",
      "Epoch 4/10\n",
      "277/277 [==============================] - 9s 31ms/step - loss: 1.5553\n",
      "Epoch 5/10\n",
      "277/277 [==============================] - 9s 32ms/step - loss: 1.4762\n",
      "Epoch 6/10\n",
      "277/277 [==============================] - 9s 31ms/step - loss: 1.4055\n",
      "Epoch 7/10\n",
      "277/277 [==============================] - 9s 31ms/step - loss: 1.3427\n",
      "Epoch 8/10\n",
      "277/277 [==============================] - 9s 32ms/step - loss: 1.2868\n",
      "Epoch 9/10\n",
      "277/277 [==============================] - 9s 32ms/step - loss: 1.2369\n",
      "Epoch 10/10\n",
      "277/277 [==============================] - 9s 32ms/step - loss: 1.1923\n",
      "175.27707862854004\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss=nll)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=1000, use_partitioning=True,\n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 132s 2s/step\n",
      "Time taken: 131.93697667121887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(789.7641892553088, 0.08769968002895012, 0.0, 0.23776788335453566, 20438)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 297s 6s/step\n",
      "Time taken: 297.1006762981415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(664.6330365006361,\n",
       " 0.1936046545792393,\n",
       " 0.13240043057050593,\n",
       " 0.3117477248263039,\n",
       " 20438)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_nightly",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
