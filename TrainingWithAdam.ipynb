{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "import time\n",
    "assert(tf.__version__.startswith('2.3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import EmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8000),\n",
    "    tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8000)])\n",
    "\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "logical_gpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "full_dataset = load_fb15k_237()\n",
    "\n",
    "rel_to_idx, ent_to_idx = create_mappings(full_dataset['train'])\n",
    "train_dataset = to_idx(full_dataset['train'], ent_to_idx, rel_to_idx)\n",
    "test_dataset = to_idx(full_dataset['test'], ent_to_idx, rel_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the unique entities and permute it before bucketing it\n",
    "unique_ents = np.random.permutation(np.array(list(set(train_dataset[:,0]).union(set(train_dataset[:,2])))))\n",
    "unique_ents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n buckets of nodes\n",
    "num_buckets = 4\n",
    "\n",
    "dataset_df = pd.DataFrame(train_dataset, columns=['s','p', 'o'])\n",
    "p_triples_bool = dict()\n",
    "p_triples = dict()\n",
    "\n",
    "# store entities in buckets\n",
    "bucketed_entities = dict()\n",
    "\n",
    "\n",
    "p_triples_multiple_buckets = dict()\n",
    "p_ent_multiple_buckets = dict()\n",
    "start_time = time.time()\n",
    "\n",
    "# Max entities in a bucket \n",
    "max_entities = unique_ents.shape[0]//num_buckets\n",
    "for i in range(num_buckets):\n",
    "    # store the entities in the buckets\n",
    "    bucketed_entities[i] = unique_ents[i * max_entities: (i+1) * max_entities]\n",
    "\n",
    "total_partitions = 0\n",
    "# partition the edges based on bucketed entities\n",
    "# if you have 2 buckets (0, 1) then you will have 3 partitions i.e.\n",
    "#   a. edges with sub and obj in bucket 0 (0 - 0)\n",
    "#   b. edges with sub and obj in bucket 1 (1 - 1)\n",
    "#   c. edges with sub in 0 and obj in  1 (0 - 1)\n",
    "for i in range(num_buckets):\n",
    "    for j in range(i, num_buckets):\n",
    "        try:\n",
    "            # based on where the triples start and end, put in respective partitions\n",
    "            p_triples_multiple_buckets[i][j] =  train_dataset[np.logical_or(\n",
    "                                                    np.logical_and(dataset_df['s'].isin(bucketed_entities[i]).values,\n",
    "                                                                  dataset_df['o'].isin(bucketed_entities[j]).values),\n",
    "                                                    np.logical_and(dataset_df['s'].isin(bucketed_entities[j]).values,\n",
    "                                                                  dataset_df['o'].isin(bucketed_entities[i]).values)), :]\n",
    "\n",
    "\n",
    "\n",
    "            p_ent_multiple_buckets[i][j] = np.array(list(set(p_triples_multiple_buckets[i][j][:, 0]).union(\n",
    "                set(p_triples_multiple_buckets[i][j][:, 2]))))\n",
    "        except KeyError:\n",
    "            p_triples_multiple_buckets[i] = dict()\n",
    "            p_ent_multiple_buckets[i] = dict()\n",
    "            p_triples_multiple_buckets[i][j] =  train_dataset[np.logical_or(\n",
    "                                                    np.logical_and(dataset_df['s'].isin(bucketed_entities[i]).values,\n",
    "                                                                  dataset_df['o'].isin(bucketed_entities[j]).values),\n",
    "                                                    np.logical_and(dataset_df['s'].isin(bucketed_entities[j]).values,\n",
    "                                                                  dataset_df['o'].isin(bucketed_entities[i]).values)), :]\n",
    "\n",
    "            p_ent_multiple_buckets[i][j] = np.array(list(set(p_triples_multiple_buckets[i][j][:, 0]).union(\n",
    "                set(p_triples_multiple_buckets[i][j][:, 2]))))\n",
    "        print('{} -> {} : {} triples, {} entities'.format(i, j, p_triples_multiple_buckets[i][j].shape,\n",
    "                                                         p_ent_multiple_buckets[i][j].shape))\n",
    "        total_partitions +=1\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print('Time Taken: {} secs'.format(end_time - start_time) )\n",
    "print('Total node partitions:', num_buckets)\n",
    "print('Total edge partitions:', total_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would go into separate classes, like we had in ampligraph 1 (for loss functions and initializers)\n",
    "# initializer\n",
    "def xavier(in_shape, out_shape):\n",
    "    std = np.sqrt(2 / (in_shape + out_shape))\n",
    "    return np.random.normal(0, std, size=(in_shape, out_shape)).astype(np.float32)\n",
    "\n",
    "#loss function\n",
    "with tf.device('GPU:0'):\n",
    "    def nll(scores_pred, eta):\n",
    "        scores_neg = scores_pred[1]\n",
    "        scores_pos = scores_pred[0]\n",
    "\n",
    "        scores_neg_reshaped = tf.reshape(scores_neg, [eta, tf.shape(scores_pos)[0]])\n",
    "        neg_exp = tf.exp(scores_neg_reshaped)\n",
    "        pos_exp = tf.exp(scores_pos)\n",
    "        softmax_score = pos_exp / (tf.reduce_sum(neg_exp, axis=0) + pos_exp)\n",
    "\n",
    "        loss = -tf.reduce_sum(tf.math.log(softmax_score))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is responsible for training the embeddings on multiple GPUs\n",
    "class DistributedTrainer():\n",
    "    \n",
    "    # this class manages the partitions\n",
    "    # This would later be responsible for persisting the input data and creating the partitions\n",
    "    # during training time, it will load the partitions and related embeddings \n",
    "    class PartitionManager():\n",
    "        def __init__(self, num_buckets, k, num_entities, num_rels, num_devices=1):\n",
    "            self.num_buckets = num_buckets\n",
    "            \n",
    "            if num_buckets > 1:\n",
    "                self.num_partitions = np.sum(np.arange(1, num_buckets+1))\n",
    "            else:\n",
    "                self.num_partitions = num_buckets\n",
    "            \n",
    "            # needs to go into the database\n",
    "            self.entity_embeddings = xavier(num_entities, k)\n",
    "            self.rel_embeddings = xavier(num_rels, k)\n",
    "            \n",
    "            self.num_devices = 1\n",
    "            \n",
    "            # use multiple GPUs only if we do multiple partitions\n",
    "            if self.num_partitions > 1:\n",
    "                self.num_devices = num_devices\n",
    "            \n",
    "        \n",
    "        def get_next_partition(self):\n",
    "            # get the next partition to train on, along with the embeddings of nodes in that partition\n",
    "            for i in range(len(p_ent_multiple_buckets)):\n",
    "                for j in range(len(p_ent_multiple_buckets[i])):\n",
    "                    partition_dict = dict(zip(p_ent_multiple_buckets[i][i+j], \n",
    "                                              np.arange(p_ent_multiple_buckets[i][i+j].shape[0])))\n",
    "                    new_rel_dict = dict(zip(np.arange(len(rel_to_idx)), np.arange(len(rel_to_idx))))\n",
    "                    # remap the triples to reflect the position in the embedding matrix\n",
    "                    remapped_triples = to_idx(p_triples_multiple_buckets[i][i+j], partition_dict, new_rel_dict)\n",
    "                    yield partition_dict, \\\n",
    "                            new_rel_dict, \\\n",
    "                            remapped_triples, \\\n",
    "                            self.entity_embeddings[list(partition_dict.keys()), :], \\\n",
    "                            self.rel_embeddings[list(new_rel_dict.keys()), :]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self, batch_size, max_ent_size, k, num_ents, num_rels, num_buckets=1, num_devices=1):\n",
    "        # max_ent_size - is the max embeddings that can be loaded in memory\n",
    "        self.partition_manager = self.PartitionManager(num_buckets, k, num_ents, num_rels, num_devices)\n",
    "        self.eta = 5\n",
    "        self.models = None # would be a list later (one per device)\n",
    "        self.optimizers = None # would be a list later (one per device)\n",
    "        self.max_ent_size = max_ent_size\n",
    "        self.k = k\n",
    "        self.batch_size=batch_size\n",
    "        self.num_devices = num_devices\n",
    "        self.num_ents = num_ents\n",
    "        self.num_rels = num_rels\n",
    "        \n",
    "        # create embedding model and optimizer\n",
    "        for i in range(num_devices):\n",
    "            with tf.device('GPU:{}'.format(i)):\n",
    "                self.optimizers = tf.optimizers.Adam(lr=0.01)\n",
    "                self.models = EmbeddingModel(eta=self.eta, \n",
    "                                             k=k, \n",
    "                                             max_ent_size=max_ent_size, \n",
    "                                             max_rel_size=self.num_rels)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        # this is the hyperparams of the optimizer - would be later moved \n",
    "        # to database calls and retrieved from partition manager\n",
    "        self.optimizer_hyperparams_ent = np.zeros(shape=(self.num_ents, 2, k), \n",
    "                                                  dtype=np.float32)\n",
    "        self.optimizer_hyperparams_rel = np.zeros(shape=(self.num_rels, 2, k), \n",
    "                                                  dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def train_dataset_generator(self, dataset):\n",
    "        # generator for the training data\n",
    "        batch_count = dataset.shape[0]//self.batch_size + 1\n",
    "        for j in range(batch_count):\n",
    "            inputs = dataset[j * self.batch_size : (j+1) * self.batch_size, :].astype(np.int32)\n",
    "            yield inputs\n",
    "            \n",
    "    def update_partion_embeddings_after_train(self):\n",
    "        # before changing the partition, save the trained embeddings and optimizer params\n",
    "        \n",
    "        self.partition_manager.entity_embeddings[list(self.ent_dict.keys()), :] = self.models.encoding_layer.ent_emb.numpy()[:len(self.ent_dict), :]\n",
    "        self.partition_manager.rel_embeddings[list(self.rel_dict.keys()), :] = self.models.encoding_layer.rel_emb.numpy()[:len(self.rel_dict), :]\n",
    "        \n",
    "        opt_weights = self.optimizers.get_weights()\n",
    "        if len(opt_weights)>0:\n",
    "            self.optimizer_hyperparams_rel[list(self.rel_dict.keys()), :, :] = np.concatenate([opt_weights[2][:len(self.rel_dict)][:, np.newaxis, :], \n",
    "                                                                                                 opt_weights[4][:len(self.rel_dict)][:, np.newaxis, :]], 1)\n",
    "            \n",
    "            self.optimizer_hyperparams_ent[list(self.ent_dict.keys()), :, :] = np.concatenate([opt_weights[1][:len(self.ent_dict)][:, np.newaxis, :], \n",
    "                                                                                             opt_weights[3][:len(self.ent_dict)][:, np.newaxis, :]], 1)\n",
    "        \n",
    "        \n",
    "    def change_partition(self):\n",
    "        # load a new partition and update the trainable params and optimizer hyperparams\n",
    "        self.ent_dict, self.rel_dict, remapped_triples, ent_embs, rel_embs = next(self.partition_iterator)\n",
    "        print('partition has {} triples', remapped_triples.shape)\n",
    "        self.partition_dataset_iterator = iter(self.train_dataset_generator(remapped_triples))\n",
    "        self.models.partition_change_updates(len(self.ent_dict), ent_embs, rel_embs)\n",
    "        if self.global_epoch >1:\n",
    "            # needs to be better handled\n",
    "            optimizer_rel_weights_updates_beta1 = self.optimizer_hyperparams_rel[list(self.rel_dict.keys()), 0, :]\n",
    "            optimizer_rel_weights_updates_beta2 = self.optimizer_hyperparams_rel[list(self.rel_dict.keys()), 1, :]\n",
    "            optimizer_ent_weights_updates_beta1 = self.optimizer_hyperparams_ent[list(self.ent_dict.keys()), 0, :]\n",
    "            optimizer_ent_weights_updates_beta2 = self.optimizer_hyperparams_ent[list(self.ent_dict.keys()), 1, :]\n",
    "            \n",
    "            optimizer_rel_weights_updates_beta1 = np.pad(optimizer_rel_weights_updates_beta1, \n",
    "                                                         ((0, self.num_rels - optimizer_rel_weights_updates_beta1.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            optimizer_rel_weights_updates_beta2 = np.pad(optimizer_rel_weights_updates_beta2, \n",
    "                                                         ((0, self.num_rels - optimizer_rel_weights_updates_beta2.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            optimizer_ent_weights_updates_beta1 = np.pad(optimizer_ent_weights_updates_beta1, \n",
    "                                                         ((0, self.max_ent_size - optimizer_ent_weights_updates_beta1.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            optimizer_ent_weights_updates_beta2 = np.pad(optimizer_ent_weights_updates_beta2, \n",
    "                                                         ((0, self.max_ent_size - optimizer_ent_weights_updates_beta2.shape[0]), \n",
    "                                                          (0,0)), \n",
    "                                                         'constant', \n",
    "                                                         constant_values=(0))\n",
    "            \n",
    "            self.optimizers.set_weights(self.optimizers.get_weights())\n",
    "\n",
    "            self.optimizers.set_weights([self.optimizers.iterations.numpy(), \n",
    "                                         optimizer_ent_weights_updates_beta1,\n",
    "                                         optimizer_rel_weights_updates_beta1,\n",
    "                                         optimizer_ent_weights_updates_beta2,\n",
    "                                         optimizer_rel_weights_updates_beta2\n",
    "                                        ])\n",
    "\n",
    "            \n",
    "    def get_next_batch(self):\n",
    "        try:\n",
    "            self.partition_iterator = iter(self.partition_manager.get_next_partition())\n",
    "            # get new partition\n",
    "            self.change_partition()\n",
    "            while True:\n",
    "                try:\n",
    "                    # get batches from the current partition\n",
    "                    out = next(self.partition_dataset_iterator)\n",
    "                    yield out\n",
    "                except StopIteration:\n",
    "                    # if no more batch data - save the trained params and load next partition\n",
    "                    self.update_partion_embeddings_after_train()\n",
    "                    self.change_partition()\n",
    "        except StopIteration:\n",
    "            #if no more partitions, end\n",
    "            return\n",
    "                \n",
    "    \n",
    "    @tf.function()\n",
    "    def train_step(self, inputs, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get the model predictions\n",
    "            preds = self.models(inputs, training=0)\n",
    "            # compute the loss\n",
    "            loss = nll(preds, self.eta)\n",
    "            # regularizer - will be in a separate class like ampligraph 1\n",
    "            loss += (0.0001 * (tf.reduce_sum(tf.pow(tf.abs(self.models.encoding_layer.ent_emb), 3)) + \\\n",
    "                              tf.reduce_sum(tf.pow(tf.abs(self.models.encoding_layer.rel_emb), 3))))\n",
    "\n",
    "        # compute the grads\n",
    "        gradients = tape.gradient(loss, [self.models.encoding_layer.ent_emb, \n",
    "                                         self.models.encoding_layer.rel_emb])\n",
    "        # update the trainable params\n",
    "        optimizer.apply_gradients(zip(gradients, [self.models.encoding_layer.ent_emb, \n",
    "                                                  self.models.encoding_layer.rel_emb]))   \n",
    "        return loss\n",
    "        \n",
    "                \n",
    "\n",
    "    def train(self, epochs = 100):\n",
    "        dataset = tf.data.Dataset.from_generator(self.get_next_batch,\n",
    "                                             output_types=(tf.int32),\n",
    "                                             output_shapes=((None, 3)))\n",
    "        dataset = dataset.prefetch(0)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(epochs):\n",
    "            total_loss = []\n",
    "            print(i)\n",
    "            self.global_epoch = i\n",
    "\n",
    "            for j, inputs in dataset.enumerate():\n",
    "                self.global_batch = 0\n",
    "                with tf.device('{}'.format('GPU:0')):\n",
    "                    loss = self.train_step(inputs, self.optimizers)\n",
    "                \n",
    "                total_loss.append(loss/inputs.shape[0])\n",
    "            \n",
    "            print('\\n\\n\\n\\nloss------------------{}:{}'.format(i, np.mean(total_loss)))\n",
    "        print('done')\n",
    "\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ents = len(ent_to_idx)\n",
    "num_rels = len(rel_to_idx)\n",
    "dist_trainer = DistributedTrainer(30000, 7000, k=300, \n",
    "                                  num_ents=num_ents, num_rels=num_rels, \n",
    "                                  num_buckets=num_buckets, num_devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dist_trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 partition\n",
    "# loss 0.05109425261616707\n",
    "# 65 sec\n",
    "\n",
    "# 10 partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_nightly",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
