{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-dev20201208\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "tf.config.set_soft_device_placement(False)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "import numpy as np\n",
    "from ampligraph.datasets import load_fb15k_237, load_yago3_10\n",
    "from ampligraph.evaluation.protocol import create_mappings, to_idx\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "import time\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith('2.5'))\n",
    "\n",
    "from ampligraph.datasets import load_fb15k_237, load_fb13, load_fb15k, load_wn11, load_wn18, load_wn18rr, load_yago3_10\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump to \n",
    "- [Partitioned](#Training/eval-with-partition)\n",
    "- [Discovery](#Discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 2s 65ms/step - loss: 16389.1543\n",
      "\n",
      "Epoch 00001: loss improved from inf to 16389.15430, saving model to ./chkpt1\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 10464.8750\n",
      "\n",
      "Epoch 00002: loss improved from 16389.15430 to 10464.87500, saving model to ./chkpt1\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 7459.9829\n",
      "\n",
      "Epoch 00003: loss improved from 10464.87500 to 7459.98291, saving model to ./chkpt1\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 5856.6294\n",
      "\n",
      "Epoch 00004: loss improved from 7459.98291 to 5856.62939, saving model to ./chkpt1\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 4870.1465\n",
      "\n",
      "Epoch 00005: loss improved from 5856.62939 to 4870.14648, saving model to ./chkpt1\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 4196.1782\n",
      "\n",
      "Epoch 00006: loss improved from 4870.14648 to 4196.17822, saving model to ./chkpt1\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 3709.9500\n",
      "\n",
      "Epoch 00007: loss improved from 4196.17822 to 3709.94995, saving model to ./chkpt1\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 3341.0066\n",
      "\n",
      "Epoch 00008: loss improved from 3709.94995 to 3341.00659, saving model to ./chkpt1\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 3052.2170\n",
      "\n",
      "Epoch 00009: loss improved from 3341.00659 to 3052.21704, saving model to ./chkpt1\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 2818.6941\n",
      "\n",
      "Epoch 00010: loss improved from 3052.21704 to 2818.69409, saving model to ./chkpt1\n",
      "Time taken: 8.481643915176392\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.01)\n",
    "# optim = 'adam'\n",
    "\n",
    "# loss = nll\n",
    "# loss = 'self_adversarial'\n",
    "from ampligraph.latent_features.loss_functions import SelfAdversarialLoss, NLLMulticlass\n",
    "loss = SelfAdversarialLoss({'margin': 0.1, 'alpha': 5, 'reduction': 'sum'})\n",
    "loss = NLLMulticlass({'reduction': 'sum'})\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim, loss=loss)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('./chkpt1', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'],\n",
    "         callbacks=[checkpoint])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "272115 triples containing invalid keys skipped!\n",
      "2/2 [==============================] - 2s 1s/step\n"
     ]
    }
   ],
   "source": [
    "ranks = model.evaluate(np.array([['/m/01cr28', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/07tw_b', '/location/country/form_of_government', '/m/02lkcc'],\n",
    "                     ['/m/073tm9', '/location/country/form_of_government', '/m/02lkcc']]), \n",
    "                       use_filter={'train': dataset['train']}, \n",
    "                       corrupt_side='s,o', \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  128,     4],\n",
       "       [ 9137, 14157],\n",
       "       [11342, 14494]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 57s 278ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.964608816909678"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=['/m/08966', '/m/05lf_', '/m/0f8l9c', '/m/04ghz4m'],\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 70s 342ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22008795596891753"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 75s 363ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22008795596891753"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "                       entities_subset=list(model.data_indexer.entities_dict.values()),\n",
    "                      \n",
    "                      use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "mrr_score(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[-2.6117418 -2.5582657 -2.4347577 ... 30.218773  31.553265  41.35157  ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "pred = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(np.sort(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "model.calibrate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                batch_size=10000, positive_base_rate=0.9, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[0.4154276  0.41815612 0.4244767  ... 0.9985715  0.9989201  0.9998617 ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "[-2.6117418 -2.5582657 -2.4347577 ... 30.218773  31.553265  41.35157  ]\n",
      "[ 4066   611 18634 ...   990 10437 14612]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "print(np.sort(out))\n",
    "print(np.argsort(out))\n",
    "pred_out = model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')\n",
    "print(np.sort(pred_out))\n",
    "print(np.argsort(pred_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 69s 337ms/step\n",
      "Time taken: 69.48330020904541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.09822389666306,\n",
       " 0.22008806659524746,\n",
       " 0.13516488893238085,\n",
       " 0.3928711224190234,\n",
       " 20438)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./calibrated_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "206/206 [==============================] - 70s 338ms/step\n",
      "Time taken: 69.74180722236633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298.09822389666306,\n",
       " 0.22008806659524746,\n",
       " 0.13516488893238085,\n",
       " 0.3928711224190234,\n",
       " 20438)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "loaded_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='ComplEx')\n",
    "loaded_model.load_weights('./calibrated_model')\n",
    "ranks = loaded_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100,\n",
    "                       corrupt_side='s,o',\n",
    "         use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                  'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                  'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "(20438,)\n",
      "[14.611029   9.369467   7.550659  ...  5.5516806  0.9343725  5.4705057]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=100)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([ 4066,   611, 18634, ...,   990, 10437, 14612]),\n",
       " array([0.4154276 , 0.41815612, 0.4244767 , ..., 0.9985715 , 0.9989201 ,\n",
       "        0.9998617 ], dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = loaded_model.predict_proba('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', batch_size=10000)\n",
    "np.argsort(out), np.argsort(loaded_model.predict('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt')), np.sort(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with RandomEdges partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n",
    "dataset_loader = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt', \n",
    "                                 backend=SQLiteAdapter,\n",
    "                                    batch_size=1000, \n",
    "                                    dataset_type='train', \n",
    "                                     use_filter=False,\n",
    "                                    use_indexer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 896.0Bytes, after: 12.968MB, consumed: 12.968MB; exec time: 31.836s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Choose the partitioner \n",
    "partitioner = PARTITION_ALGO_REGISTRY.get('RandomEdges')(dataset_loader, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "274/274 [==============================] - 18s 67ms/step - loss: 1.5924\n",
      "Epoch 2/2\n",
      "274/274 [==============================] - 18s 66ms/step - loss: 1.4184\n",
      "48.908854484558105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit(partitioner,\n",
    "                     batch_size=1000, use_partitioning=True,             \n",
    "                     epochs=2)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=SQLiteAdapter,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=partitioned_model.data_handler.get_mapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 128s 2s/step\n",
      "Time taken: 128.1078543663025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1664.727125941873, 0.08627633556309791, 0.0, 0.23722967022213523, 20438)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_model.save_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_part_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "loaded_part_model.load_weights('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_loader_test = GraphDataLoader('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                        backend=DummyBackend,\n",
    "                                        batch_size=400, \n",
    "                                        dataset_type='test', \n",
    "                                        use_indexer=loaded_part_model.data_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 134s 3s/step\n",
      "Time taken: 134.12054896354675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1664.727125941873, 0.08627633556309791, 0.0, 0.23722967022213523, 20438)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = loaded_part_model.evaluate(dataset_loader_test, \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/eval with partition (default Partitioning Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import DummyBackend, SQLiteAdapter\n",
    "from ampligraph.datasets import GraphDataLoader\n",
    "from ampligraph.datasets.graph_partitioner import PARTITION_ALGO_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 102.39MB, after: 112.47MB, consumed: 10.081MB; exec time: 107.22s\n",
      "Epoch 1/10\n",
      "277/277 [==============================] - 15s 53ms/step - loss: 1.7248\n",
      "Epoch 2/10\n",
      "277/277 [==============================] - 15s 54ms/step - loss: 1.6170\n",
      "Epoch 3/10\n",
      "277/277 [==============================] - 15s 53ms/step - loss: 1.5097\n",
      "Epoch 4/10\n",
      "277/277 [==============================] - 15s 54ms/step - loss: 1.4106\n",
      "Epoch 5/10\n",
      "277/277 [==============================] - 15s 54ms/step - loss: 1.3221\n",
      "Epoch 6/10\n",
      "277/277 [==============================] - 15s 54ms/step - loss: 1.2444\n",
      "Epoch 7/10\n",
      "277/277 [==============================] - 15s 53ms/step - loss: 1.1767\n",
      "Epoch 8/10\n",
      "277/277 [==============================] - 15s 53ms/step - loss: 1.1174\n",
      "Epoch 9/10\n",
      "277/277 [==============================] - 15s 53ms/step - loss: 1.0654\n",
      "Epoch 10/10\n",
      "277/277 [==============================] - 15s 53ms/step - loss: 1.0194\n",
      "266.2339057922363\n"
     ]
    }
   ],
   "source": [
    "optim = tf.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300, \n",
    "                                     scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer=optim, loss='multiclass_nll')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "partitioned_model.fit('/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                     batch_size=1000, use_partitioning=True,\n",
    "                     epochs=10)\n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 175s 3s/step\n",
      "Time taken: 174.85773611068726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(662.5323172521773, 0.08724452354269595, 0.0, 0.23862413151971817, 20438)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                                   batch_size=400)\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "\n",
      "9 triples containing invalid keys skipped!\n",
      "\n",
      "28 triples containing invalid keys skipped!\n",
      "53/53 [==============================] - 334s 6s/step\n",
      "Time taken: 334.2117545604706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(535.9683677463548,\n",
       " 0.19239034593139454,\n",
       " 0.12875525981015756,\n",
       " 0.31578432331930717,\n",
       " 20438)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "ranks = partitioned_model.evaluate('/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt', \n",
    "                       batch_size=400,\n",
    "                       corrupt_side='s,o',\n",
    "                        use_filter={'train':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/train.txt',\n",
    "                              'valid':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/valid.txt',\n",
    "                              'test':'/home/spai/code/ampligraph_projects/dataset/fb15k-237/test.txt'})\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 1.7917\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.7902\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.7797\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.7368\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.6371\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.4968\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 1.3558\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.2324\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.1280\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 1.0398\n",
      "Time taken: 7.4232001304626465\n",
      "\n",
      "100 triples containing invalid keys skipped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['/m/028_yv', '/location/country/form_of_government', '/m/0m313']],\n",
       "       dtype=object),\n",
       " array([59.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ampligraph.discovery import discover_facts\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                     k=300,\n",
    "                                     scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "start = time.time()\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             validation_freq=50,\n",
    "             validation_batch_size=100,\n",
    "             validation_data = dataset['valid'])\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', end-start)\n",
    "\n",
    "discover_facts(dataset['train'][:100], \n",
    "               model, \n",
    "               top_n=100, \n",
    "               strategy='random_uniform', \n",
    "               max_candidates=100, \n",
    "               target_rel='/location/country/form_of_government', \n",
    "               seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 2s 43ms/step - loss: 1.7917\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.7904\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.7790\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.7389\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 1.6729\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.5834\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.4727\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.3564\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.2489\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 1.1549\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "# International football matches triples\n",
    "# See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "open('football.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                                 k=300,\n",
    "                                 scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "model.fit(X,\n",
    "          batch_size=10000,\n",
    "          epochs=10)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "                               df.o[df.o.str.startswith(\"Team\")])))\n",
    "team_embeddings = model.get_embeddings(teams, embedding_type='e')\n",
    "\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "\n",
    "# Find clusters of embeddings using KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "clusters = find_clusters(teams, model, kmeans, mode='e')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 1.7918\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7915\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7912\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.7908\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7902\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7893\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.7882\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7866\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7844\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7816\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7778\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7731\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7672\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7599\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.7512\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7409\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7289\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.7152\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.6997\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.6825\n",
      "                                             movie_name  year\n",
      "1200                                           Cornered  2011\n",
      "1199                                           Cornered  2011\n",
      "727                                             Captive  2013\n",
      "728                                             Captive  2013\n",
      "2222  One Piece: Episode of Alabaster - Sabaku no Oj...  2007\n",
      "2223  One Piece: Episode of Alabaster - Sabaku no Oj...  2007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "# The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "import requests\n",
    "url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "import tarfile\n",
    "tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Reading tabular dataset of IMDB movies and filling the missing values\n",
    "imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "\n",
    "# Creating knowledge graph triples from tabular dataset\n",
    "imdb_triples = []\n",
    "\n",
    "for _, row in imdb.iterrows():\n",
    "    movie_id = \"ID\" + str(row[\"id\"])\n",
    "    directors = row[\"directors\"].split(\",\")\n",
    "    actors = row[\"actors\"].split(\",\")\n",
    "    genres = row[\"genre\"].split(\",\")\n",
    "    duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "\n",
    "    directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "\n",
    "    imdb_triples.extend(directors_triples)\n",
    "    imdb_triples.extend(actors_triples)\n",
    "    imdb_triples.extend(genres_triples)\n",
    "    imdb_triples.append(duration_triple)\n",
    "\n",
    "# Training knowledge graph embedding with ComplEx model\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=300,\n",
    "                             scoring_type='ComplEx')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "imdb_triples = np.array(imdb_triples)\n",
    "model.fit(imdb_triples,\n",
    "      batch_size=10000,\n",
    "      epochs=20)\n",
    "\n",
    "# Finding duplicates movies (entities)\n",
    "from ampligraph.discovery import find_duplicates\n",
    "\n",
    "entities = np.unique(imdb_triples[:, 0])\n",
    "dups, _ = find_duplicates(entities, model, mode='e', tolerance=0.45)\n",
    "id_list = []\n",
    "for data in dups:\n",
    "    for i in data:\n",
    "        id_list.append(int(i[2:]))\n",
    "print(imdb.iloc[id_list[:6]][['movie_name', 'year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 4.9561\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.9071\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 4.8704\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.8381\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 4.8098\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 0s 10ms/step - loss: 4.7835\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4.7591\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.7362\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.7149\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.6942\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 0s 13ms/step - loss: 4.6748\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 4.6557\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 4.6374\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 4.6194\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 4.6025\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.5860\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 4.5695\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 4.5536\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 4.5384\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 4.5237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['Eddard Stark', 'ALLIED_WITH', 'House Baelish of the Fingers'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'Larys Strong'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'Harlan Hunter'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Dalt of Lemonwood'],\n",
       "        ['Eddard Stark', 'ALLIED_WITH', 'House Tarbeck of Tarbeck Hall']],\n",
       "       dtype='<U44'),\n",
       " array([-1.3960195, -1.3976465, -1.4125104, -1.4159653, -1.4202976],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.discovery import discover_facts\n",
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "# Game of Thrones relations dataset\n",
    "url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/GoT.csv'\n",
    "open('GoT.csv', 'wb').write(requests.get(url).content)\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')\n",
    "\n",
    "model = ScoringBasedEmbeddingModel(eta=5, \n",
    "                             k=150,\n",
    "                             scoring_type='TransE')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adagrad', loss='pairwise')\n",
    "model.fit(X,\n",
    "      batch_size=100,\n",
    "      epochs=20)\n",
    "query_topn(model, top_n=5,\n",
    "        head='Eddard Stark', relation='ALLIED_WITH', tail=None,\n",
    "        ents_to_consider=None, rels_to_consider=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_nightly",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
